{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOPRbInO8G7SsO74TZCv+DR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yiwenwangANU/pytorch_review/blob/main/Extra_Shakespeare_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import os\n",
        "from pathlib import Path\n",
        "import requests"
      ],
      "metadata": {
        "id": "I4gOLJiLRYzy"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(\"GPU ready!\")\n",
        "else:\n",
        "  device = \"cpu\"\n",
        "  print(\"GPU not available!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGyCf3JSTMTf",
        "outputId": "f43ebc36-91eb-4746-e5a2-866bf80c51fa"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data download and explore"
      ],
      "metadata": {
        "id": "7xIk_As9rnN7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RP-67rJQ2Ia",
        "outputId": "6ae2f1ee-9b3b-4b9a-89a1-e98d8a112152"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data ready.\n"
          ]
        }
      ],
      "source": [
        "data_dir = Path.home() / 'data'\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "data_dir = os.path.join(data_dir, \"input.txt\")\n",
        "if os.path.exists(data_dir):\n",
        "  print(\"Data ready.\")\n",
        "else:\n",
        "  print(f\"Downloading training data to {data_dir}\")\n",
        "  url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "  torch.hub.download_url_to_file(url, data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(data_dir, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "    print(f\"length of dataset in characters: {len(text)}\")\n",
        "    print('First 300 chars:')\n",
        "    print(text[:300])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvqsUA7vSodf",
        "outputId": "b26f2373-7028-4251-9b85-d199bff4f951"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1115394\n",
            "First 300 chars:\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenization"
      ],
      "metadata": {
        "id": "3mfoK1i8r11M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(list(set(text)))\n",
        "vocab_size = len(vocab)\n",
        "print(f'Number of unique chars: {vocab_size}')\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuK1Lw5kUavB",
        "outputId": "aa5c1295-b249-4a9d-fae5-281b2eaaff8b"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique chars: 65\n",
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "charTotoken = {char:i for i,char in enumerate(vocab)}\n",
        "tokenTochar = {i:char for i,char in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "_dJSU6ztYafO"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(input):\n",
        "  return [charTotoken[char] for char in input]\n",
        "\n",
        "def decode(input):\n",
        "  return ''.join([tokenTochar[token] for token in input])"
      ],
      "metadata": {
        "id": "nw8M80vrUU3n"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encode('First Citizen:'))\n",
        "print(decode(encode('First Citizen:')))\n",
        "# Check other tokenizer like gpt-tokenizer"
      ],
      "metadata": {
        "id": "YEuMJXdrbDSA",
        "outputId": "2abe4859-c435-4406-b7d7-19d80b3f4190",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10]\n",
            "First Citizen:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape)\n",
        "print(data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcqcRGKOptr0",
        "outputId": "bd3c99ab-1356-455c-9675-0759152f0611"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394])\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train test split and create training set"
      ],
      "metadata": {
        "id": "ABWZPmCwqOm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "Rr3679IUsCnS"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create training/validation set by randomly sample sequence at block_size in training data\n",
        "def get_sample(data_set, batch_size, block_size):\n",
        "  idxes = torch.randint(len(data_set)-block_size, (batch_size,))\n",
        "  xb = torch.stack([data_set[idx:idx+block_size] for idx in idxes])\n",
        "  yb = torch.stack([data_set[idx+1: idx+block_size+1] for idx in idxes])\n",
        "  return xb, yb"
      ],
      "metadata": {
        "id": "DkpMEUOOIyJD"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "block_size = 8\n",
        "\n",
        "xb, yb = get_sample(train_data, batch_size, block_size)\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "id": "zj71XSdR3Il7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ff749b0-f674-4681-ce02-94d48564ae34"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[ 1, 50, 47, 43, 57, 58,  8,  0],\n",
            "        [ 0, 19, 24, 27, 33, 15, 17, 31],\n",
            "        [41, 46,  1, 41, 53, 52, 57, 59],\n",
            "        [59, 52, 42, 56, 43, 42,  1, 57]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[50, 47, 43, 57, 58,  8,  0, 32],\n",
            "        [19, 24, 27, 33, 15, 17, 31, 32],\n",
            "        [46,  1, 41, 53, 52, 57, 59, 51],\n",
            "        [52, 42, 56, 43, 42,  1, 57, 46]])\n",
            "----\n",
            "when input is [1] the target: 50\n",
            "when input is [1, 50] the target: 47\n",
            "when input is [1, 50, 47] the target: 43\n",
            "when input is [1, 50, 47, 43] the target: 57\n",
            "when input is [1, 50, 47, 43, 57] the target: 58\n",
            "when input is [1, 50, 47, 43, 57, 58] the target: 8\n",
            "when input is [1, 50, 47, 43, 57, 58, 8] the target: 0\n",
            "when input is [1, 50, 47, 43, 57, 58, 8, 0] the target: 32\n",
            "when input is [0] the target: 19\n",
            "when input is [0, 19] the target: 24\n",
            "when input is [0, 19, 24] the target: 27\n",
            "when input is [0, 19, 24, 27] the target: 33\n",
            "when input is [0, 19, 24, 27, 33] the target: 15\n",
            "when input is [0, 19, 24, 27, 33, 15] the target: 17\n",
            "when input is [0, 19, 24, 27, 33, 15, 17] the target: 31\n",
            "when input is [0, 19, 24, 27, 33, 15, 17, 31] the target: 32\n",
            "when input is [41] the target: 46\n",
            "when input is [41, 46] the target: 1\n",
            "when input is [41, 46, 1] the target: 41\n",
            "when input is [41, 46, 1, 41] the target: 53\n",
            "when input is [41, 46, 1, 41, 53] the target: 52\n",
            "when input is [41, 46, 1, 41, 53, 52] the target: 57\n",
            "when input is [41, 46, 1, 41, 53, 52, 57] the target: 59\n",
            "when input is [41, 46, 1, 41, 53, 52, 57, 59] the target: 51\n",
            "when input is [59] the target: 52\n",
            "when input is [59, 52] the target: 42\n",
            "when input is [59, 52, 42] the target: 56\n",
            "when input is [59, 52, 42, 56] the target: 43\n",
            "when input is [59, 52, 42, 56, 43] the target: 42\n",
            "when input is [59, 52, 42, 56, 43, 42] the target: 1\n",
            "when input is [59, 52, 42, 56, 43, 42, 1] the target: 57\n",
            "when input is [59, 52, 42, 56, 43, 42, 1, 57] the target: 46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 unique embeddings and use 3 to represent each token\n",
        "embedding = nn.Embedding(num_embeddings=10, embedding_dim=3)\n",
        "\n",
        "# Input tensor (indices of words or items)\n",
        "input_indices = torch.tensor([1, 5, 7, 7])\n",
        "\n",
        "# Get embeddings\n",
        "output = embedding(input_indices)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "ZdbKBaK91rL1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10b6316f-b16b-464c-e0e2-574d13a9a713"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.1477, -0.6008, -1.5328],\n",
            "        [-0.9608, -0.1654, -1.4084],\n",
            "        [-1.3594, -0.5436, -1.1195],\n",
            "        [-1.3594, -0.5436, -1.1195]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bigram Model"
      ],
      "metadata": {
        "id": "FClvLSICxN39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# A Bigram Model predicts the next token based on the embedding of previous token\n",
        "# (i.e., it learns a probability distribution of token transitions).\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    # each token directly reads off the logits for the next token from a lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "  def forward(self, x):\n",
        "    return self.token_embedding_table(x)\n",
        "\n",
        "model_bigram = BigramLanguageModel(vocab_size)"
      ],
      "metadata": {
        "id": "6J0VseFUZ0d8"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the last token in x to predict next token\n",
        "# and loop to generate text at seq_len length\n",
        "torch.manual_seed(42)\n",
        "def generate(x, seq_len): # x-> shape(Batch size, 1)\n",
        "  for _ in range(seq_len):\n",
        "    logits = model_bigram(x) # shape(Batch size, Sequence length, Vocabulary size)\n",
        "    logits_last = logits[:,-1,:] # shape(Batch size, Vocabulary size)\n",
        "    logits_last_prob = F.softmax(logits_last, dim=-1) # shape(Batch size, Vocabulary size)\n",
        "    token_next = torch.multinomial(logits_last_prob, num_samples=1) # shape(B, 1)\n",
        "    x = torch.cat((x, token_next), dim=1) # (B, T+1)\n",
        "  return x"
      ],
      "metadata": {
        "id": "VBMGVktAtpUu"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate use untrained model to generate text\n",
        "new_text = generate(torch.zeros(1, 1, dtype=torch.long), seq_len=100)\n",
        "print(new_text)\n",
        "print(decode(new_text.squeeze().numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxHyDoZ8xa-t",
        "outputId": "65d66817-b530-41f8-f832-2faf4fcaa3b9"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0, 25, 60, 15, 19, 27, 61, 34, 35, 42, 40, 49, 49, 17, 51, 50, 12, 35,\n",
            "          7, 11,  8, 50, 49, 28, 38,  7, 23, 54, 25, 17, 59, 28, 30, 30, 49, 17,\n",
            "          4, 28, 43,  8, 63, 49,  7, 46, 15, 33, 59, 60, 49, 16, 62, 42, 24,  9,\n",
            "         62, 62, 17, 25, 59, 28, 40, 38, 28, 19, 37, 12, 46, 31,  7, 52, 15, 40,\n",
            "         39, 55, 59, 36, 46, 15,  9, 56, 16, 21,  7, 54, 64, 29, 14, 50,  5, 26,\n",
            "         62, 42, 34, 28, 40, 47, 56, 39, 55, 35, 42]])\n",
            "\n",
            "MvCGOwVWdbkkEml?W-;.lkPZ-KpMEuPRRkE&Pe.yk-hCUuvkDxdL3xxEMuPbZPGY?hS-nCbaquXhC3rDI-pzQBl'NxdVPbiraqWd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params=model_bigram.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "f8fYdRc_suHq"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoches = 1000\n",
        "for epoch in range(epoches):\n",
        "  model_bigram.train()\n",
        "  xb, yb = get_sample(train_data, batch_size, block_size) # get random sample in each epoch\n",
        "  logits = model_bigram(xb)\n",
        "  B, T, C = logits.shape # change the shape to match the input shape of loss_fn\n",
        "  logits = logits.view(B*T, C)\n",
        "  targets = yb.view(B*T)\n",
        "  loss = loss_fn(logits, targets)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if(epoch == 0):\n",
        "    print(f'Logits shape: {logits.shape}')\n",
        "  if(epoch % 100 == 0):\n",
        "    print(f'epoch: {epoch}, train loss: {loss.item():.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMxiNCX7taUk",
        "outputId": "ef722b7a-4cdf-4ab1-833f-e0d360429b21"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape: torch.Size([32, 65])\n",
            "epoch: 0, train loss: 4.92\n",
            "epoch: 100, train loss: 4.71\n",
            "epoch: 200, train loss: 4.48\n",
            "epoch: 300, train loss: 4.51\n",
            "epoch: 400, train loss: 4.54\n",
            "epoch: 500, train loss: 4.62\n",
            "epoch: 600, train loss: 4.43\n",
            "epoch: 700, train loss: 4.26\n",
            "epoch: 800, train loss: 4.13\n",
            "epoch: 900, train loss: 4.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate use trained model to generate text\n",
        "print(decode(generate(torch.zeros(1, 1, dtype=torch.long), seq_len=500).squeeze().numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oavwDrSW_9xy",
        "outputId": "1953ec76-9737-4154-c7ce-b5a9a0d92651"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CE\n",
            "Br?rczWpGY3fn WBJSGPgfff,oio\n",
            "NLQn!vTuX.yvNa3F?kaq'hjGtCjQKocN\n",
            "UpouPblMLLMptjVr,y.y xdVVzJgsdX.IW,gC$OVCODnCwcTVF&XhUKAt&xoIVF?flvwgdWKnN;3uoiaF$z\n",
            "M?kI;h\n",
            "DbuMG,H3LYNmrDxpWHHvAKOF-jU.ho;fBuOya-IS\n",
            "ghOEb&ZQ,l;:mslpcNN\n",
            "KpVEYRIIM,'hCRblAcWTo;niab&am&K3ZnkIMaqntjh-ARIaqu\n",
            "ghjZTBRS$J?qBQbwyCjhRheuyJoGtsutJBy-j&,r,'k$PTuZm3tPbu mKH-IseadVJVrw'x f -d3si'g.cu,k-JMiv?R-jlqIeairoan\n",
            "3VEmtWcEIccN\n",
            "AFDHhesYzETkUGI;f .bYh,g:gSeIKptaxTBykJG.!gsz ffgpGLHa3xDkWlrwVm3C\n",
            "Kh.H?Hvy!ogQahk!DmgiZ&X.SkeI-kooERqKpjvTWPS:Nz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Single head self-attention"
      ],
      "metadata": {
        "id": "Xoh9aH7tw6Z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## lower triangle masking\n",
        "## to remove the influence from tokens to previous ones\n",
        "B, T, C = 4, 8, 32 # batch, context/sequence length, channel/embedding\n",
        "x = torch.rand(B, T, C)\n",
        "\n",
        "def apply_mask(wei):\n",
        "  tri = torch.tril(torch.ones(T, T))\n",
        "  wei = torch.masked_fill(wei, tri==0, float('-inf')) # for text generation\n",
        "  wei = F.softmax(wei, dim=-1)\n",
        "  return wei\n",
        "\n",
        "wei = torch.zeros(T, T)\n",
        "print(apply_mask(wei))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEVfNHJ0xZQv",
        "outputId": "68a3d628-75a2-4a59-f49f-33a2b5d2408a"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## key, query and affinity matrix\n",
        "B, T, C = 4, 8, 32\n",
        "x = torch.rand(B, T, C)\n",
        "\n",
        "head_size = 16 # as embedding size\n",
        "# match each token from embedding to head that answer who I am\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "# match each token from embedding to head that ask what I want\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "k = key(x) # (B, T, head_size)\n",
        "q = query(x) # (B, T, head_size)\n",
        "\n",
        "# affinity matrix indicates the relationship between tokens\n",
        "affinity = q @ k.transpose(-2, -1) # (B, T, head) @ (B, head, T) -> (B, T, T)\n",
        "print(apply_mask(affinity)) # after apply mask and softmax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAWdAV4dxpXO",
        "outputId": "171b021a-6e07-4616-9083-6cb449257253"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.5855, 0.4145, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.3671, 0.3057, 0.3272, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.3067, 0.2027, 0.2134, 0.2772, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2399, 0.1743, 0.1903, 0.2152, 0.1803, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2356, 0.1099, 0.1793, 0.1609, 0.1562, 0.1582, 0.0000, 0.0000],\n",
            "         [0.1727, 0.1160, 0.1506, 0.1440, 0.1019, 0.1473, 0.1674, 0.0000],\n",
            "         [0.1329, 0.1086, 0.0991, 0.1404, 0.1164, 0.1108, 0.1354, 0.1564]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.5109, 0.4891, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.3849, 0.3327, 0.2825, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2558, 0.2811, 0.1516, 0.3114, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1803, 0.2208, 0.1489, 0.2042, 0.2458, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1801, 0.1627, 0.1000, 0.2306, 0.2325, 0.0941, 0.0000, 0.0000],\n",
            "         [0.1239, 0.1498, 0.0961, 0.1387, 0.2462, 0.1042, 0.1411, 0.0000],\n",
            "         [0.1380, 0.1184, 0.1049, 0.1167, 0.1414, 0.1229, 0.1257, 0.1320]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.5192, 0.4808, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2981, 0.4006, 0.3013, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2591, 0.2464, 0.2583, 0.2363, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1798, 0.1951, 0.1964, 0.1978, 0.2310, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1730, 0.1825, 0.1497, 0.1630, 0.1727, 0.1591, 0.0000, 0.0000],\n",
            "         [0.1374, 0.1764, 0.1447, 0.1377, 0.1295, 0.1503, 0.1239, 0.0000],\n",
            "         [0.1166, 0.1357, 0.1137, 0.1158, 0.1432, 0.1201, 0.1183, 0.1367]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4001, 0.5999, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.3065, 0.4083, 0.2852, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2354, 0.2571, 0.2785, 0.2290, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1850, 0.2315, 0.1970, 0.2206, 0.1659, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1561, 0.2121, 0.1536, 0.1729, 0.1643, 0.1410, 0.0000, 0.0000],\n",
            "         [0.1197, 0.1828, 0.1293, 0.1652, 0.1359, 0.1313, 0.1358, 0.0000],\n",
            "         [0.1195, 0.1368, 0.1351, 0.1277, 0.1099, 0.1403, 0.1058, 0.1250]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right) V\n",
        "$$"
      ],
      "metadata": {
        "id": "VhhTmn3H9tHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B, T, head_size)\n",
        "k.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_94B-qCu-Y7t",
        "outputId": "f9f79fb6-2aa8-40ba-e96f-38040f1f0dd1"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9660)"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q = torch.randn(B, T, head_size)\n",
        "q.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDU8YC8J-hS2",
        "outputId": "839c8f27-d30b-4820-9f77-941e61fe2141"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9567)"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "affinity = q @ k.transpose(-2, -1) * head_size ** -0.5 # to keep the var\n",
        "affinity.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "no6fiL2m-7rC",
        "outputId": "0eec8762-ea67-4e59-fb66-c90a0a5298f1"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.8634)"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# value matrix and normalization\n",
        "B, T, C = 4, 8, 32\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "# match each token from embedding to head that related what can it provide\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "k = key(x)\n",
        "q = query(x)\n",
        "v = value(x)\n",
        "\n",
        "affinity = q @ k.transpose(-2, -1) / torch.sqrt(torch.tensor(head_size))\n",
        "affinity = apply_mask(affinity) # (B, T, T)\n",
        "out = affinity @ v # (B, T, head_size)\n",
        "out[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKbPcaJC3_Xd",
        "outputId": "73bce898-b289-4fca-fd6a-c3d6e62b506e"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.4008, -0.2824, -0.0504, -0.1648,  0.2199, -0.6905,  0.3993,  0.0089,\n",
              "          0.0030,  0.3607, -0.5116, -0.2277,  0.1935, -0.4891,  0.3365,  0.0191],\n",
              "        [-0.5572, -0.3694,  0.0655, -0.1710, -0.0216, -0.5794, -0.0694,  0.0759,\n",
              "          0.1772,  0.1894,  0.0330, -0.3166, -0.1639, -0.6994,  0.6523, -0.3480],\n",
              "        [-0.3748, -0.3747,  0.0972, -0.3189,  0.0290, -0.6332,  0.0132,  0.3073,\n",
              "          0.0451,  0.0726,  0.0844, -0.3094, -0.1222, -0.4423,  0.7516, -0.3954],\n",
              "        [-0.3757, -0.1354,  0.0678, -0.3544,  0.5488, -0.5346,  0.2777,  0.4462,\n",
              "          0.4850,  0.0387,  0.4747,  0.0056, -0.1109,  0.2689,  0.5192, -0.0423],\n",
              "        [-0.2842, -0.2239,  0.1320, -0.2791,  0.2471, -0.6057,  0.1309,  0.4110,\n",
              "          0.2765, -0.0763,  0.4293, -0.1000, -0.2380, -0.0087,  0.6724, -0.3274],\n",
              "        [-0.2859, -0.1018,  0.1047, -0.1025,  0.4274, -0.4564,  0.3123,  0.2511,\n",
              "          0.3937, -0.0916,  0.3692, -0.0540, -0.0763, -0.0555,  0.3209, -0.1108],\n",
              "        [-0.5626, -0.2395,  0.0634, -0.0972,  0.4926, -0.4463,  0.2095,  0.3150,\n",
              "          0.2814, -0.1290,  0.4680, -0.0645, -0.0351, -0.1562,  0.1637, -0.1115],\n",
              "        [-0.2537, -0.0628,  0.2133, -0.2383,  0.6116, -0.4860,  0.3236,  0.2903,\n",
              "          0.2149, -0.0260,  0.4090,  0.0291, -0.0401,  0.0206,  0.0989, -0.1565]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model with single-head attention"
      ],
      "metadata": {
        "id": "NdMQhtweNAOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32 # B\n",
        "block_size = 8 # T\n",
        "embedding_dims = 32 # C\n",
        "vocab_size = 65\n",
        "epoches = 5000\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "NdCID_pyJc4k"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  # single head self attention\n",
        "  def __init__(self, block_size, embedding_dims, head_size):\n",
        "    super().__init__()\n",
        "    self.query = nn.Linear(embedding_dims, head_size)\n",
        "    self.key = nn.Linear(embedding_dims, head_size)\n",
        "    self.value = nn.Linear(embedding_dims, head_size)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "\n",
        "    q = self.query(x)\n",
        "    k = self.key(x)\n",
        "    v = self.value(x)\n",
        "    affinity = q @ k.transpose(-2, -1) / torch.sqrt(torch.tensor(head_size))\n",
        "\n",
        "    affinity = torch.masked_fill(affinity, self.tril[:T, :T] == 0, float('-inf')) # for text generation\n",
        "    affinity = F.softmax(affinity, dim=-1)\n",
        "\n",
        "    out = affinity @ v # (B, T, T) @ (B, T, head_size) -> (B, T, head_size)\n",
        "    return out"
      ],
      "metadata": {
        "id": "iXFcrCI6_dW8"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleHeadModel(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dims, block_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding = nn.Embedding(vocab_size, embedding_dims)\n",
        "    self.positon_embedding = nn.Embedding(block_size, embedding_dims)\n",
        "    self.self_attention_head = Head(block_size, embedding_dims, head_size=embedding_dims)\n",
        "    self.decoder = nn.Linear(embedding_dims, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T = x.shape\n",
        "    x = self.token_embedding(x) + self.positon_embedding(torch.arange(T, device=device)) # (B, T, C) + (T, C) -> (B, T, C)\n",
        "    x = self.self_attention_head(x) # (B, T, C)\n",
        "    x = self.decoder(x) # (B, T, vocab_size)\n",
        "    return x\n",
        "\n",
        "model_single_head = SingleHeadModel(vocab_size=vocab_size, embedding_dims=embedding_dims, block_size=block_size).to(device)"
      ],
      "metadata": {
        "id": "1EXhG592L62_"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use x(shape(B, T)) to predict next token,\n",
        "# after go through the model still use last token to predict the next token,\n",
        "# and loop to generate text at new_seq_len length\n",
        "def generate(x, new_seq_len): # x-> shape(B, T)\n",
        "  model_single_head.eval()\n",
        "  x = x.to(device)\n",
        "  for _ in range(new_seq_len):\n",
        "    with torch.inference_mode():\n",
        "      context = x[:, -block_size:] # if x exceeds block_size, only feed the last block_size tokens to the model\n",
        "      logits = model_single_head(context) # (B, T, vocab_size)\n",
        "      logits_last = logits[:,-1,:] # (B, vocab_size)\n",
        "      logits_last_prob = F.softmax(logits_last, dim=-1) # (B, vocab_size)\n",
        "      token_next = torch.multinomial(logits_last_prob, num_samples=1) # (B, 1)\n",
        "      x = torch.cat((x, token_next), dim=1) # (B, T+1)\n",
        "  return x"
      ],
      "metadata": {
        "id": "OeDUEG-gRfJz"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params=model_single_head.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "VawWeMWzH46s"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epoches):\n",
        "  model_single_head.train()\n",
        "  x_train, y_train = get_sample(train_data, batch_size, block_size) # get random sample in each epoch\n",
        "  x_train, y_train = x_train.to(device), y_train.to(device)\n",
        "  logits = model_single_head(x_train)\n",
        "  B, T, C = logits.shape # change the shape to match the input shape of loss_fn\n",
        "  logits = logits.view(B*T, C)\n",
        "  targets = y_train.view(B*T)\n",
        "  loss = loss_fn(logits, targets)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  model_single_head.eval()\n",
        "  val_loss = 0.0\n",
        "  with torch.inference_mode():\n",
        "    test_loss = 0.0\n",
        "    x_val, y_val = get_sample(val_data, batch_size, block_size) # get random sample in each epoch\n",
        "    x_val, y_val = x_train.to(device), y_train.to(device)\n",
        "    val_logits = model_single_head(x_val)\n",
        "    B, T, C = val_logits.shape # change the shape to match the input shape of loss_fn\n",
        "    val_logits = val_logits.view(B*T, C)\n",
        "    val_targets = y_val.view(B*T)\n",
        "    val_loss = loss_fn(val_logits, val_targets)\n",
        "  if(epoch == 0):\n",
        "    print(f'Logits shape: {logits.shape}')\n",
        "  if(epoch % 1000 == 0):\n",
        "    print(f'epoch: {epoch}, train loss: {loss.item():.4f}, val loss: {val_loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gke25i7jH_hs",
        "outputId": "d47fa9a7-5683-4be5-9455-9a62605fb279"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape: torch.Size([256, 65])\n",
            "epoch: 0, train loss: 2.3565, val loss: 2.3519\n",
            "epoch: 1000, train loss: 2.5141, val loss: 2.5082\n",
            "epoch: 2000, train loss: 2.6513, val loss: 2.6451\n",
            "epoch: 3000, train loss: 2.2744, val loss: 2.2686\n",
            "epoch: 4000, train loss: 2.3394, val loss: 2.3345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate use trained model to generate text\n",
        "print(decode(generate(torch.zeros(1, 1, dtype=torch.long), new_seq_len=500).squeeze().cpu().numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVdhwazBLMWQ",
        "outputId": "1deb976a-bf2c-4deb-fcff-2bd43865733f"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "IFO: as; nte ty hn my thur mod thand he, preckn,\n",
            "OH:\n",
            "D\n",
            "TUESY:\n",
            "Sow lapiwisers we hakergoeptend muprt, ody inde eve ditlat mang yofo thy\n",
            "SThind.\n",
            "\n",
            "Wheer url hat ght me machan fl Coust dieere--sbwurore cense wince me, woess thee?\n",
            "\n",
            "Firomest;\n",
            "Al me hming I hee lars ole!-\n",
            "I ind hnerve she, f'd ban ndowr:\n",
            "Lit ot t's lore ry, bd ph-e\n",
            "peleme thildosos I hearung.\n",
            "\n",
            "COLOREV ORER:\n",
            "Tom, ban te ssefr I wos blodu'es cemnode bs as GmR:\n",
            "Kef DI basy Vo on my.\n",
            "\n",
            "IRWINIIONUnstiloru!\n",
            "\n",
            "INIUCHake I eald barsit ut wntier.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Multi-head self-attention model"
      ],
      "metadata": {
        "id": "3S9d10vfVOi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32 # B\n",
        "block_size = 8 # T\n",
        "embedding_dims = 32 # C\n",
        "vocab_size = 65\n",
        "num_heads = 4\n",
        "epoches = 5000\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "T7cP0DU2Wsen"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module): # in -> (B, T, C) out -> (B, T, head_size)\n",
        "  # single head self attention\n",
        "  def __init__(self, block_size, embedding_dims, head_size):\n",
        "    super().__init__()\n",
        "    self.query = nn.Linear(embedding_dims, head_size)\n",
        "    self.key = nn.Linear(embedding_dims, head_size)\n",
        "    self.value = nn.Linear(embedding_dims, head_size)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "\n",
        "    q = self.query(x)\n",
        "    k = self.key(x)\n",
        "    v = self.value(x)\n",
        "    affinity = q @ k.transpose(-2, -1) / torch.sqrt(torch.tensor(head_size))\n",
        "\n",
        "    affinity = torch.masked_fill(affinity, self.tril[:T, :T] == 0, float('-inf')) # for text generation\n",
        "    affinity = F.softmax(affinity, dim=-1)\n",
        "\n",
        "    out = affinity @ v # (B, T, T) @ (B, T, head_size) -> (B, T, head_size)\n",
        "    return out"
      ],
      "metadata": {
        "id": "lF8RcFO7YuUG"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHead(nn.Module): # in -> (B, T, C) out -> (B, T, num_heads * head_size)\n",
        "  def __init__(self, num_heads, block_size, embedding_dims, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(block_size, embedding_dims, head_size) for _ in range(num_heads)])\n",
        "  def forward(self, x):\n",
        "    return torch.cat([head(x) for head in self.heads], dim=-1)"
      ],
      "metadata": {
        "id": "n_BpLN08ZBJo"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module): # in (B, T, C) out -> (B, T, C)\n",
        "  def __init__(self, embedding_dims):\n",
        "    super().__init__()\n",
        "    self.layer_stack = nn.Sequential(\n",
        "        nn.Linear(embedding_dims, embedding_dims), # add feedforward network\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(embedding_dims, embedding_dims)\n",
        "        )\n",
        "  def forward(self, x):\n",
        "    return self.layer_stack(x)"
      ],
      "metadata": {
        "id": "zVhEsBaSjhYI"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(nn.Module): # in (B, T, C) out -> (B, T, C)\n",
        "  def __init__(self, num_heads, block_size, embedding_dims):\n",
        "    super().__init__()\n",
        "    head_size = embedding_dims // num_heads\n",
        "    self.self_attention = MultiHead(num_heads, block_size, embedding_dims, head_size)\n",
        "    self.feedforward = FeedForward(embedding_dims)\n",
        "  def forward(self, x): # x shape (B, T, C)\n",
        "    x = self.self_attention(x)\n",
        "    x = self.feedforward(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "rfpNkTIJlZyY"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadModel(nn.Module):\n",
        "  def __init__(self, num_heads, vocab_size, embedding_dims, block_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding = nn.Embedding(vocab_size, embedding_dims)\n",
        "    self.positon_embedding = nn.Embedding(block_size, embedding_dims)\n",
        "    self.attention_blocks = nn.Sequential(\n",
        "        AttentionBlock(num_heads, block_size, embedding_dims),\n",
        "        AttentionBlock(num_heads, block_size, embedding_dims),\n",
        "        AttentionBlock(num_heads, block_size, embedding_dims),\n",
        "    )\n",
        "    self.linear = nn.Linear(embedding_dims, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T = x.shape\n",
        "    x = self.token_embedding(x) + self.positon_embedding(torch.arange(T, device=device)) # (B, T, C) + (T, C) -> (B, T, C)\n",
        "    x = self.attention_blocks(x) # (B, T, C)\n",
        "    x = self.linear(x) # (B, T, vocab_size)\n",
        "    return x\n",
        "\n",
        "model_multi_head = MultiHeadModel(num_heads=num_heads,\n",
        "                                  vocab_size=vocab_size,\n",
        "                                  embedding_dims=embedding_dims,\n",
        "                                  block_size=block_size).to(device)"
      ],
      "metadata": {
        "id": "UCiKCyyCazAb"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params=model_multi_head.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "1bL7HNPHb5q8"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epoches):\n",
        "  model_multi_head.train()\n",
        "  x_train, y_train = get_sample(train_data, batch_size, block_size) # get random sample in each epoch\n",
        "  x_train, y_train = x_train.to(device), y_train.to(device)\n",
        "  logits = model_multi_head(x_train)\n",
        "  B, T, C = logits.shape # change the shape to match the input shape of loss_fn\n",
        "  logits = logits.view(B*T, C)\n",
        "  targets = y_train.view(B*T)\n",
        "  loss = loss_fn(logits, targets)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  model_multi_head.eval()\n",
        "  val_loss = 0.0\n",
        "  with torch.inference_mode():\n",
        "    test_loss = 0.0\n",
        "    x_val, y_val = get_sample(val_data, batch_size, block_size) # get random sample in each epoch\n",
        "    x_val, y_val = x_train.to(device), y_train.to(device)\n",
        "    val_logits = model_multi_head(x_val)\n",
        "    B, T, C = val_logits.shape # change the shape to match the input shape of loss_fn\n",
        "    val_logits = val_logits.view(B*T, C)\n",
        "    val_targets = y_val.view(B*T)\n",
        "    val_loss = loss_fn(val_logits, val_targets)\n",
        "  if(epoch == 0):\n",
        "    print(f'Logits shape: {logits.shape}')\n",
        "  if(epoch % 1000 == 0):\n",
        "    print(f'epoch: {epoch}, train loss: {loss.item():.4f}, val loss: {val_loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9EmYG1cchze",
        "outputId": "1a6ee0ea-4250-4681-8245-d6dbac25b329"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape: torch.Size([256, 65])\n",
            "epoch: 0, train loss: 4.1529, val loss: 4.1456\n",
            "epoch: 1000, train loss: 2.7593, val loss: 2.7370\n",
            "epoch: 2000, train loss: 2.5666, val loss: 2.5541\n",
            "epoch: 3000, train loss: 2.4018, val loss: 2.3912\n",
            "epoch: 4000, train loss: 2.3907, val loss: 2.3790\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(x, new_seq_len): # x-> shape(B, T)\n",
        "  model_multi_head.eval()\n",
        "  x = x.to(device)\n",
        "  for _ in range(new_seq_len):\n",
        "    with torch.inference_mode():\n",
        "      context = x[:, -block_size:] # if x exceeds block_size, only feed the last block_size tokens to the model\n",
        "      logits = model_multi_head(context) # (B, T, vocab_size)\n",
        "      logits_last = logits[:,-1,:] # (B, vocab_size)\n",
        "      logits_last_prob = F.softmax(logits_last, dim=-1) # (B, vocab_size)\n",
        "      token_next = torch.multinomial(logits_last_prob, num_samples=1) # (B, 1)\n",
        "      x = torch.cat((x, token_next), dim=1) # (B, T+1)\n",
        "  return x"
      ],
      "metadata": {
        "id": "yKLOGxYWexEf"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate use trained model to generate text\n",
        "print(decode(generate(torch.zeros(1, 1, dtype=torch.long), new_seq_len=500).squeeze().cpu().numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46AH4QMWcpxo",
        "outputId": "24d127f7-cb18-4593-f2b3-78645a41cff1"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Wour ghok earnf therer enl: by dot it\n",
            "And shubancllelice.\n",
            "\n",
            "KFOORVYD:\n",
            "To hod had mnerpveme hearver thy fipu blaonge sbomm liten nois butaxnse rear'n; thate noser buslece thilgood liek leroparebiredsy Vrowting erbe none; py ckise wirty: ther ler hatuasst gakes theardjrerpangd you'sm,'k treabuchoe, meid: ther, wou! the boveray, as.\n",
            "\n",
            "I thother; cherst woursarmnsesemong mee'd tnesk you moud bearourtmordfr the trrer thur jomem siir frededfecoder? shaciles confamidely domernn, thel, Tow, minealtondaed,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make model deeper and applying tricks that to make sure network in optimizable\n",
        "1.  add residual connection\n",
        "2.  layer norm\n",
        "3.  dropout layer"
      ],
      "metadata": {
        "id": "ylLq65LLez_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64 # B\n",
        "block_size = 256 # T\n",
        "embedding_dims = 384 # C\n",
        "vocab_size = 65\n",
        "num_heads = 6\n",
        "n_layers = 6\n",
        "dropout = 0.2\n",
        "\n",
        "epoches = 5000\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "jvbSZc_0rRCN"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module): # in -> (B, T, C) out -> (B, T, head_size)\n",
        "  # single head self attention\n",
        "  def __init__(self, block_size, embedding_dims, head_size):\n",
        "    super().__init__()\n",
        "    self.query = nn.Linear(embedding_dims, head_size)\n",
        "    self.key = nn.Linear(embedding_dims, head_size)\n",
        "    self.value = nn.Linear(embedding_dims, head_size)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "\n",
        "    q = self.query(x)\n",
        "    k = self.key(x)\n",
        "    v = self.value(x)\n",
        "    affinity = q @ k.transpose(-2, -1) / torch.sqrt(torch.tensor(head_size))\n",
        "\n",
        "    affinity = torch.masked_fill(affinity, self.tril[:T, :T] == 0, float('-inf')) # for text generation\n",
        "    affinity = F.softmax(affinity, dim=-1)\n",
        "    affinity = self.dropout(affinity) # dropout the affinity matrix\n",
        "    out = affinity @ v # (B, T, T) @ (B, T, head_size) -> (B, T, head_size)\n",
        "    return out"
      ],
      "metadata": {
        "id": "0Ck83beDsHvT"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHead(nn.Module): # in -> (B, T, C) out -> (B, T, num_heads * head_size)\n",
        "  def __init__(self, num_heads, block_size, embedding_dims, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(block_size, embedding_dims, head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(embedding_dims, embedding_dims) # residual connection\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  def forward(self, x):\n",
        "    x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "    x = self.proj(x)\n",
        "    x = self.dropout(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "FRJneabOsKp8"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module): # in (B, T, C) out -> (B, T, C)\n",
        "  def __init__(self, embedding_dims):\n",
        "    super().__init__()\n",
        "    self.layer_stack = nn.Sequential(\n",
        "        nn.Linear(embedding_dims, 4 * embedding_dims), # add feedforward network\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * embedding_dims, embedding_dims), # projection layer that go back to residual pathway\n",
        "        nn.Dropout(dropout) # add dropout after residual path way\n",
        "        )\n",
        "  def forward(self, x):\n",
        "    return self.layer_stack(x)"
      ],
      "metadata": {
        "id": "YtNS3WlWsNN7"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(nn.Module): # in (B, T, C) out -> (B, T, C)\n",
        "  def __init__(self, num_heads, block_size, embedding_dims):\n",
        "    super().__init__()\n",
        "    head_size = embedding_dims // num_heads\n",
        "    self.self_attention = MultiHead(num_heads, block_size, embedding_dims, head_size)\n",
        "    self.feedforward = FeedForward(embedding_dims)\n",
        "    self.layernorm1 = nn.LayerNorm(embedding_dims)\n",
        "    self.layernorm2 = nn.LayerNorm(embedding_dims)\n",
        "  def forward(self, x): # x shape (B, T, C)\n",
        "    x = x + self.self_attention(self.layernorm1(x)) # add residual connection and layer norm\n",
        "    x = x + self.feedforward(self.layernorm2(x)) # add residual connection\n",
        "    return x"
      ],
      "metadata": {
        "id": "utcrtn4RsPyj"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadModel(nn.Module):\n",
        "  def __init__(self, num_heads, vocab_size, embedding_dims, block_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding = nn.Embedding(vocab_size, embedding_dims)\n",
        "    self.positon_embedding = nn.Embedding(block_size, embedding_dims)\n",
        "    self.attention_blocks = nn.Sequential(\n",
        "      *[AttentionBlock(num_heads, block_size, embedding_dims) for _ in range(n_layers)]\n",
        "      )\n",
        "    self.layernorm = nn.LayerNorm(embedding_dims) # add layer norm\n",
        "    self.linear = nn.Linear(embedding_dims, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T = x.shape\n",
        "    x = self.token_embedding(x) + self.positon_embedding(torch.arange(T, device=device)) # (B, T, C) + (T, C) -> (B, T, C)\n",
        "    x = self.attention_blocks(x)\n",
        "    x = self.layernorm(x)\n",
        "    x = self.linear(x) # (B, T, vocab_size)\n",
        "    return x\n",
        "\n",
        "model_multi_head = MultiHeadModel(num_heads=num_heads,\n",
        "                                  vocab_size=vocab_size,\n",
        "                                  embedding_dims=embedding_dims,\n",
        "                                  block_size=block_size).to(device)"
      ],
      "metadata": {
        "id": "Ub8Eaeg8sSGT"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params=model_multi_head.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "yY3gi4UUx-_V"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epoches):\n",
        "  model_multi_head.train()\n",
        "  x_train, y_train = get_sample(train_data, batch_size, block_size) # get random sample in each epoch\n",
        "  x_train, y_train = x_train.to(device), y_train.to(device)\n",
        "  logits = model_multi_head(x_train)\n",
        "  B, T, C = logits.shape # change the shape to match the input shape of loss_fn\n",
        "  logits = logits.view(B*T, C)\n",
        "  targets = y_train.view(B*T)\n",
        "  loss = loss_fn(logits, targets)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  model_multi_head.eval()\n",
        "  val_loss = 0.0\n",
        "  with torch.inference_mode():\n",
        "    test_loss = 0.0\n",
        "    x_val, y_val = get_sample(val_data, batch_size, block_size) # get random sample in each epoch\n",
        "    x_val, y_val = x_train.to(device), y_train.to(device)\n",
        "    val_logits = model_multi_head(x_val)\n",
        "    B, T, C = val_logits.shape # change the shape to match the input shape of loss_fn\n",
        "    val_logits = val_logits.view(B*T, C)\n",
        "    val_targets = y_val.view(B*T)\n",
        "    val_loss = loss_fn(val_logits, val_targets)\n",
        "  if(epoch == 0):\n",
        "    print(f'Logits shape: {logits.shape}')\n",
        "  if(epoch % 1000 == 0):\n",
        "    print(f'epoch: {epoch}, train loss: {loss.item():.4f}, val loss: {val_loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2SdOM4SyBYf",
        "outputId": "8ceb0c5e-1bb6-4c7b-90b6-47cfef9b9469"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape: torch.Size([16384, 65])\n",
            "epoch: 0, train loss: 4.3724, val loss: 3.9283\n",
            "epoch: 1000, train loss: 1.4644, val loss: 1.3727\n",
            "epoch: 2000, train loss: 1.2678, val loss: 1.1781\n",
            "epoch: 3000, train loss: 1.1298, val loss: 1.0222\n",
            "epoch: 4000, train loss: 1.0539, val loss: 0.9323\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(x, new_seq_len): # x-> shape(B, T)\n",
        "  model_multi_head.eval()\n",
        "  x = x.to(device)\n",
        "  for _ in range(new_seq_len):\n",
        "    with torch.inference_mode():\n",
        "      context = x[:, -block_size:] # if x exceeds block_size, only feed the last block_size tokens to the model\n",
        "      logits = model_multi_head(context) # (B, T, vocab_size)\n",
        "      logits_last = logits[:,-1,:] # (B, vocab_size)\n",
        "      logits_last_prob = F.softmax(logits_last, dim=-1) # (B, vocab_size)\n",
        "      token_next = torch.multinomial(logits_last_prob, num_samples=1) # (B, 1)\n",
        "      x = torch.cat((x, token_next), dim=1) # (B, T+1)\n",
        "  return x"
      ],
      "metadata": {
        "id": "pI8ImfSJyEJg"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate use trained model to generate text\n",
        "print(decode(generate(torch.zeros(1, 1, dtype=torch.long), new_seq_len=2000).squeeze().cpu().numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXUZddvD6Zw3",
        "outputId": "89042c72-ff8a-41a4-f525-62c2f93bfc93"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Or seem'st a bodk indeed great proper bench\n",
            "And that's before his limbs.\n",
            "\n",
            "COMINIUS:\n",
            "Do you think oft mine: thus but is an end\n",
            "The chamber of birth countenance guilty of\n",
            "The spirits and language of him what we heard the brother\n",
            "Laid's prenzie.\n",
            "\n",
            "MARCIUS:\n",
            "There is a present death to our mortal lady,\n",
            "Our soldier vice to instruct me of our\n",
            "business to bite in the spirit of custom, the state\n",
            "And strength o' the melancholy brains\n",
            "All reasons of near, the nice o' the time;\n",
            "The earth of the royalties and kindly be here,\n",
            "Ten thyself and all the sovereigns were still shed\n",
            "He was the prevail of his livery.\n",
            "\n",
            "LEONTES:\n",
            "Nothing but he won that; only of my master\n",
            "Appear it, he is sent in my sex tongue!\n",
            "\n",
            "PAULINA:\n",
            "Is true? Is this truth?\n",
            "\n",
            "LEONTES:\n",
            "Ay, as the imagine of him? My cause.\n",
            "\n",
            "LEONTES:\n",
            "No, I promised well for that.\n",
            "\n",
            "LEONTES:\n",
            "My reasons for thee:\n",
            "For a friar good Lord ever made her;\n",
            "Behold, hold, am I, that, unless\n",
            "Dead, where that he profits like him beaten an\n",
            "The state and do choke of children rich would quickly like\n",
            "A vile below and a little thing;\n",
            "And yet sweetly together of thine orator,\n",
            "The meaner for a fair poison of those I am.\n",
            "Landering thy blood, he's such first; and though with vile pin\n",
            "A citizens a mother made age a fool\n",
            "And drop not so, store not the weights for me.\n",
            "\n",
            "KING HENRY VI:\n",
            "What fatest fellow? or what shall be sworn?\n",
            "\n",
            "Second Huntsman:\n",
            "I do beseech you, sir, and he will not have said\n",
            "it yielding defiance; and the mere with your woes against\n",
            "him, whose hand upon it.\n",
            "\n",
            "First Murderer:\n",
            "'Zounds, o' the senator's claim i' the next way\n",
            "The offence you and granted with child the foul discourse,\n",
            "Your ancient queen and you'll stand be patient,\n",
            "And, some man i' the house of Polixenes:\n",
            "O, fellow, I pray, pretty use life, from Oxford,\n",
            "Be arrival to earth in the great seat of our queen,\n",
            "And he will I trim his majesty to a bear.\n",
            "\n",
            "GLOUCESTER:\n",
            "So, 'tis a cowardly and virtuous: and ay, and I did\n",
            "Save it not; that was broken to break\n",
            "The trick o' the truth: this is a man way,\n",
            "\n"
          ]
        }
      ]
    }
  ]
}