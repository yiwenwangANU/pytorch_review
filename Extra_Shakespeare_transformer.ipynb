{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMiJ37seDI85P//7QfmvcbP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yiwenwangANU/pytorch_review/blob/main/Extra_Shakespeare_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import os\n",
        "from pathlib import Path\n",
        "import requests"
      ],
      "metadata": {
        "id": "I4gOLJiLRYzy"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(\"GPU ready!\")\n",
        "else:\n",
        "  device = \"cpu\"\n",
        "  print(\"GPU not available!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGyCf3JSTMTf",
        "outputId": "c21a5d38-2c45-4332-8b6d-ac356edfea51"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU not available!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data download and explore"
      ],
      "metadata": {
        "id": "7xIk_As9rnN7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RP-67rJQ2Ia",
        "outputId": "4c8e7507-dd93-43eb-a774-fbec57692960"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading training data to /root/data/input.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.06M/1.06M [00:00<00:00, 16.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "data_dir = Path.home() / 'data'\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "data_dir = os.path.join(data_dir, \"input.txt\")\n",
        "if os.path.exists(data_dir):\n",
        "  print(\"Data ready.\")\n",
        "else:\n",
        "  print(f\"Downloading training data to {data_dir}\")\n",
        "  url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "  torch.hub.download_url_to_file(url, data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(data_dir, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "    print(f\"length of dataset in characters: {len(text)}\")\n",
        "    print('First 300 chars:')\n",
        "    print(text[:300])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvqsUA7vSodf",
        "outputId": "778d436b-db6e-4206-c964-08a781ad313f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1115394\n",
            "First 300 chars:\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenization"
      ],
      "metadata": {
        "id": "3mfoK1i8r11M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(list(set(text)))\n",
        "vocab_size = len(vocab)\n",
        "print(f'Number of unique chars: {vocab_size}')\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuK1Lw5kUavB",
        "outputId": "5159dd76-1c98-4c7b-a421-88888d2e3142"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique chars: 65\n",
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "charTotoken = {char:i for i,char in enumerate(vocab)}\n",
        "tokenTochar = {i:char for i,char in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "_dJSU6ztYafO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(input):\n",
        "  return [charTotoken[char] for char in input]\n",
        "\n",
        "def decode(input):\n",
        "  return ''.join([tokenTochar[token] for token in input])"
      ],
      "metadata": {
        "id": "nw8M80vrUU3n"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encode('First Citizen:'))\n",
        "print(decode(encode('First Citizen:')))"
      ],
      "metadata": {
        "id": "YEuMJXdrbDSA",
        "outputId": "978b71dc-a67b-4f65-8ce1-5f8db8b8f515",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10]\n",
            "First Citizen:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check other tokenizer like gpt-tokenizer"
      ],
      "metadata": {
        "id": "MrmzVgsabEL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape)\n",
        "print(data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcqcRGKOptr0",
        "outputId": "61009d23-2883-4722-d5d5-0f5628877112"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394])\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train test split"
      ],
      "metadata": {
        "id": "ABWZPmCwqOm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "Rr3679IUsCnS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "\n",
        "def get_sample(batch_size, block_size):\n",
        "  idxes = torch.randint(len(train_data)-block_size, (batch_size,))\n",
        "  xb = torch.stack([train_data[idx:idx+block_size] for idx in idxes])\n",
        "  yb = torch.stack([train_data[idx+1: idx+block_size+1] for idx in idxes])\n",
        "  return xb, yb\n",
        "\n",
        "xb, yb = get_sample(batch_size, block_size)\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "id": "zj71XSdR3Il7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "995f661c-8167-48a6-fcc1-b9baa9288717"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[57,  1, 46, 47, 57,  1, 50, 53],\n",
            "        [ 1, 58, 46, 43, 56, 43,  1, 41],\n",
            "        [17, 26, 15, 17, 10,  0, 32, 53],\n",
            "        [57, 58,  6,  1, 61, 47, 58, 46]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[ 1, 46, 47, 57,  1, 50, 53, 60],\n",
            "        [58, 46, 43, 56, 43,  1, 41, 39],\n",
            "        [26, 15, 17, 10,  0, 32, 53,  1],\n",
            "        [58,  6,  1, 61, 47, 58, 46,  0]])\n",
            "----\n",
            "when input is [57] the target: 1\n",
            "when input is [57, 1] the target: 46\n",
            "when input is [57, 1, 46] the target: 47\n",
            "when input is [57, 1, 46, 47] the target: 57\n",
            "when input is [57, 1, 46, 47, 57] the target: 1\n",
            "when input is [57, 1, 46, 47, 57, 1] the target: 50\n",
            "when input is [57, 1, 46, 47, 57, 1, 50] the target: 53\n",
            "when input is [57, 1, 46, 47, 57, 1, 50, 53] the target: 60\n",
            "when input is [1] the target: 58\n",
            "when input is [1, 58] the target: 46\n",
            "when input is [1, 58, 46] the target: 43\n",
            "when input is [1, 58, 46, 43] the target: 56\n",
            "when input is [1, 58, 46, 43, 56] the target: 43\n",
            "when input is [1, 58, 46, 43, 56, 43] the target: 1\n",
            "when input is [1, 58, 46, 43, 56, 43, 1] the target: 41\n",
            "when input is [1, 58, 46, 43, 56, 43, 1, 41] the target: 39\n",
            "when input is [17] the target: 26\n",
            "when input is [17, 26] the target: 15\n",
            "when input is [17, 26, 15] the target: 17\n",
            "when input is [17, 26, 15, 17] the target: 10\n",
            "when input is [17, 26, 15, 17, 10] the target: 0\n",
            "when input is [17, 26, 15, 17, 10, 0] the target: 32\n",
            "when input is [17, 26, 15, 17, 10, 0, 32] the target: 53\n",
            "when input is [17, 26, 15, 17, 10, 0, 32, 53] the target: 1\n",
            "when input is [57] the target: 58\n",
            "when input is [57, 58] the target: 6\n",
            "when input is [57, 58, 6] the target: 1\n",
            "when input is [57, 58, 6, 1] the target: 61\n",
            "when input is [57, 58, 6, 1, 61] the target: 47\n",
            "when input is [57, 58, 6, 1, 61, 47] the target: 58\n",
            "when input is [57, 58, 6, 1, 61, 47, 58] the target: 46\n",
            "when input is [57, 58, 6, 1, 61, 47, 58, 46] the target: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = nn.Embedding(num_embeddings=10, embedding_dim=3)\n",
        "\n",
        "# Input tensor (indices of words or items)\n",
        "input_indices = torch.tensor([1, 5, 7, 7])\n",
        "\n",
        "# Get embeddings\n",
        "output = embedding(input_indices)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "ZdbKBaK91rL1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e16fe368-ac43-437a-a983-d344be6abc1b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.1914, -0.8140, -0.7360],\n",
            "        [-0.3387, -1.3407, -0.5854],\n",
            "        [ 0.0516,  0.7440, -0.4816],\n",
            "        [ 0.0516,  0.7440, -0.4816]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# A Bigram Model predicts the next token based only on the previous token\n",
        "# (i.e., it learns a probability distribution of token transitions).\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    # each token directly reads off the logits for the next token from a lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "  def forward(self, x):\n",
        "    return self.token_embedding_table(x)\n",
        "\n",
        "model_bigram = BigramLanguageModel(vocab_size)"
      ],
      "metadata": {
        "id": "6J0VseFUZ0d8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the last token in x to predict next token\n",
        "# and loop to generate text at seq_len length\n",
        "torch.manual_seed(42)\n",
        "def generate(x, seq_len): # x-> shape(Batch size, 1)\n",
        "  for _ in range(seq_len):\n",
        "    logits = model_bigram(x) # shape(Batch size, Sequence length, Vocabulary size)\n",
        "    logits_last = logits[:,-1,:] # shape(Batch size, Vocabulary size)\n",
        "    logits_last_prob = F.softmax(logits_last, dim=-1) # shape(Batch size, Vocabulary size)\n",
        "    token_next = torch.multinomial(logits_last_prob, num_samples=1) # shape(B, 1)\n",
        "    x = torch.cat((x, token_next), dim=1) # (B, T+1)\n",
        "  return x"
      ],
      "metadata": {
        "id": "VBMGVktAtpUu"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate use untrained model to generate text\n",
        "new_text = generate(torch.zeros(1, 1, dtype=torch.long), seq_len=100)\n",
        "print(new_text)\n",
        "print(decode(new_text.squeeze().numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxHyDoZ8xa-t",
        "outputId": "452c8796-1ceb-4083-ec44-85ed274f098c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0, 53, 49, 22, 55, 59, 20,  7, 25, 56, 64, 54, 42, 62, 45, 49, 21, 58,\n",
            "         16, 35, 55, 20, 39, 45, 56, 37,  9, 37, 49, 17, 30,  0, 61, 48, 15, 32,\n",
            "          5, 32, 61,  0, 63, 54, 38,  5, 25, 18, 12, 35, 15, 24, 45, 46, 23, 40,\n",
            "          0, 61, 52, 23, 54, 47, 48, 18,  2, 21,  7, 23, 54, 16, 54, 42, 49, 18,\n",
            "         37, 12, 35, 41,  4, 49, 17, 11,  8, 60,  9, 62, 52, 41, 25, 13,  0, 55,\n",
            "         52, 10, 14, 12, 12, 49, 51, 23,  5, 20, 33]])\n",
            "\n",
            "okJquH-MrzpdxgkItDWqHagrY3YkER\n",
            "wjCT'Tw\n",
            "ypZ'MF?WCLghKb\n",
            "wnKpijF!I-KpDpdkFY?Wc&kE;.v3xncMA\n",
            "qn:B??kmK'HU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params=model_bigram.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "f8fYdRc_suHq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoches = 100\n",
        "for epoch in range(epoches):\n",
        "  model_bigram.train()\n",
        "  xb, yb = get_sample(batch_size, block_size) # get random sample in each epoch\n",
        "  logits = model_bigram(xb)\n",
        "  B, T, C = logits.shape # change the shape to match the input shape of loss_fn\n",
        "  logits = logits.view(B*T, C)\n",
        "  targets = yb.view(B*T)\n",
        "  loss = loss_fn(logits, targets)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if(epoch == 0):\n",
        "    print(f'Logits shape: {logits.shape}')\n",
        "  if(epoch % 20 == 0):\n",
        "    print(f'epoch: {epoch}, train loss: {loss.item():.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMxiNCX7taUk",
        "outputId": "6bdced56-8edc-4606-920e-217efcef49c6"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape: torch.Size([32, 65])\n",
            "epoch: 0, train loss: 4.61\n",
            "epoch: 20, train loss: 4.18\n",
            "epoch: 40, train loss: 4.37\n",
            "epoch: 60, train loss: 4.36\n",
            "epoch: 80, train loss: 4.33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate use trained model to generate text\n",
        "print(decode(generate(torch.zeros(1, 1, dtype=torch.long), seq_len=100).squeeze().numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oavwDrSW_9xy",
        "outputId": "6b512369-01dc-45b7-ae30-72c62918b3c1"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "wVJTKvTwn'!MVxTVT'ryMq 3niU.WvoMFUr:JJc&eF$RIKQVaqK:mfB?pcNwEmDOtnxFjvTwyk- dGmRjYxDJPGIIMCa&?ZeOrBW\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WwTqhXd7tNXv"
      }
    }
  ]
}