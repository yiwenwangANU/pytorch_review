{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNLhMiMrthMV2uZeVDjvy12",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yiwenwangANU/pytorch_review/blob/main/Extra_Shakespeare_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import os\n",
        "from pathlib import Path\n",
        "import requests"
      ],
      "metadata": {
        "id": "I4gOLJiLRYzy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(\"GPU ready!\")\n",
        "else:\n",
        "  device = \"cpu\"\n",
        "  print(\"GPU not available!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGyCf3JSTMTf",
        "outputId": "68a74340-2644-4da8-eaee-739668bfd59a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data download and explore"
      ],
      "metadata": {
        "id": "7xIk_As9rnN7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RP-67rJQ2Ia",
        "outputId": "050d8d84-388d-4066-bea5-e8eb9b74992a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading training data to /root/data/input.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.06M/1.06M [00:00<00:00, 25.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "data_dir = Path.home() / 'data'\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "data_dir = os.path.join(data_dir, \"input.txt\")\n",
        "if os.path.exists(data_dir):\n",
        "  print(\"Data ready.\")\n",
        "else:\n",
        "  print(f\"Downloading training data to {data_dir}\")\n",
        "  url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "  torch.hub.download_url_to_file(url, data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(data_dir, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "    print(f\"length of dataset in characters: {len(text)}\")\n",
        "    print('First 300 chars:')\n",
        "    print(text[:300])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvqsUA7vSodf",
        "outputId": "5f80c135-1ca9-46ef-aca9-aeb1dd7c7be1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1115394\n",
            "First 300 chars:\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenization"
      ],
      "metadata": {
        "id": "3mfoK1i8r11M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(list(set(text)))\n",
        "vocab_size = len(vocab)\n",
        "print(f'Number of unique chars: {vocab_size}')\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuK1Lw5kUavB",
        "outputId": "07ead3d4-7ab0-4c0e-82b6-a0ca5a04a203"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique chars: 65\n",
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "charTotoken = {char:i for i,char in enumerate(vocab)}\n",
        "tokenTochar = {i:char for i,char in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "_dJSU6ztYafO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(input):\n",
        "  return [charTotoken[char] for char in input]\n",
        "\n",
        "def decode(input):\n",
        "  return ''.join([tokenTochar[token] for token in input])"
      ],
      "metadata": {
        "id": "nw8M80vrUU3n"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encode('First Citizen:'))\n",
        "print(decode(encode('First Citizen:')))"
      ],
      "metadata": {
        "id": "YEuMJXdrbDSA",
        "outputId": "405a0fe6-892c-4db3-db95-0edd12afc7ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10]\n",
            "First Citizen:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check other tokenizer like gpt-tokenizer"
      ],
      "metadata": {
        "id": "MrmzVgsabEL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape)\n",
        "print(data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcqcRGKOptr0",
        "outputId": "eeac29ec-c3b6-4cd1-f254-2ca73f8aa793"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394])\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train test split"
      ],
      "metadata": {
        "id": "ABWZPmCwqOm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "Rr3679IUsCnS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "\n",
        "def get_sample(batch_size, block_size):\n",
        "  idxes = torch.randint(len(train_data)-block_size, (batch_size,))\n",
        "  xb = torch.stack([train_data[idx:idx+block_size] for idx in idxes])\n",
        "  yb = torch.stack([train_data[idx+1: idx+block_size+1] for idx in idxes])\n",
        "  return xb, yb\n",
        "\n",
        "xb, yb = get_sample(batch_size, block_size)\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "id": "zj71XSdR3Il7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "541d5dd0-0d77-4117-92a2-2197366c8a72"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[57,  1, 46, 47, 57,  1, 50, 53],\n",
            "        [ 1, 58, 46, 43, 56, 43,  1, 41],\n",
            "        [17, 26, 15, 17, 10,  0, 32, 53],\n",
            "        [57, 58,  6,  1, 61, 47, 58, 46]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[ 1, 46, 47, 57,  1, 50, 53, 60],\n",
            "        [58, 46, 43, 56, 43,  1, 41, 39],\n",
            "        [26, 15, 17, 10,  0, 32, 53,  1],\n",
            "        [58,  6,  1, 61, 47, 58, 46,  0]])\n",
            "----\n",
            "when input is [57] the target: 1\n",
            "when input is [57, 1] the target: 46\n",
            "when input is [57, 1, 46] the target: 47\n",
            "when input is [57, 1, 46, 47] the target: 57\n",
            "when input is [57, 1, 46, 47, 57] the target: 1\n",
            "when input is [57, 1, 46, 47, 57, 1] the target: 50\n",
            "when input is [57, 1, 46, 47, 57, 1, 50] the target: 53\n",
            "when input is [57, 1, 46, 47, 57, 1, 50, 53] the target: 60\n",
            "when input is [1] the target: 58\n",
            "when input is [1, 58] the target: 46\n",
            "when input is [1, 58, 46] the target: 43\n",
            "when input is [1, 58, 46, 43] the target: 56\n",
            "when input is [1, 58, 46, 43, 56] the target: 43\n",
            "when input is [1, 58, 46, 43, 56, 43] the target: 1\n",
            "when input is [1, 58, 46, 43, 56, 43, 1] the target: 41\n",
            "when input is [1, 58, 46, 43, 56, 43, 1, 41] the target: 39\n",
            "when input is [17] the target: 26\n",
            "when input is [17, 26] the target: 15\n",
            "when input is [17, 26, 15] the target: 17\n",
            "when input is [17, 26, 15, 17] the target: 10\n",
            "when input is [17, 26, 15, 17, 10] the target: 0\n",
            "when input is [17, 26, 15, 17, 10, 0] the target: 32\n",
            "when input is [17, 26, 15, 17, 10, 0, 32] the target: 53\n",
            "when input is [17, 26, 15, 17, 10, 0, 32, 53] the target: 1\n",
            "when input is [57] the target: 58\n",
            "when input is [57, 58] the target: 6\n",
            "when input is [57, 58, 6] the target: 1\n",
            "when input is [57, 58, 6, 1] the target: 61\n",
            "when input is [57, 58, 6, 1, 61] the target: 47\n",
            "when input is [57, 58, 6, 1, 61, 47] the target: 58\n",
            "when input is [57, 58, 6, 1, 61, 47, 58] the target: 46\n",
            "when input is [57, 58, 6, 1, 61, 47, 58, 46] the target: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = nn.Embedding(num_embeddings=10, embedding_dim=3)\n",
        "\n",
        "# Input tensor (indices of words or items)\n",
        "input_indices = torch.tensor([1, 5, 7, 7])\n",
        "\n",
        "# Get embeddings\n",
        "output = embedding(input_indices)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "ZdbKBaK91rL1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45d2480a-0528-44fe-e81f-0c5788a6fe3d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.6047,  1.7878, -0.4780],\n",
            "        [ 1.7174,  0.3189, -0.4245],\n",
            "        [ 0.3211, -0.8798, -0.6011],\n",
            "        [ 0.3211, -0.8798, -0.6011]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bigram Model"
      ],
      "metadata": {
        "id": "FClvLSICxN39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# A Bigram Model predicts the next token based only on the previous token\n",
        "# (i.e., it learns a probability distribution of token transitions).\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    # each token directly reads off the logits for the next token from a lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "  def forward(self, x):\n",
        "    return self.token_embedding_table(x)\n",
        "\n",
        "model_bigram = BigramLanguageModel(vocab_size)"
      ],
      "metadata": {
        "id": "6J0VseFUZ0d8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the last token in x to predict next token\n",
        "# and loop to generate text at seq_len length\n",
        "torch.manual_seed(42)\n",
        "def generate(x, seq_len): # x-> shape(Batch size, 1)\n",
        "  for _ in range(seq_len):\n",
        "    logits = model_bigram(x) # shape(Batch size, Sequence length, Vocabulary size)\n",
        "    logits_last = logits[:,-1,:] # shape(Batch size, Vocabulary size)\n",
        "    logits_last_prob = F.softmax(logits_last, dim=-1) # shape(Batch size, Vocabulary size)\n",
        "    token_next = torch.multinomial(logits_last_prob, num_samples=1) # shape(B, 1)\n",
        "    x = torch.cat((x, token_next), dim=1) # (B, T+1)\n",
        "  return x"
      ],
      "metadata": {
        "id": "VBMGVktAtpUu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate use untrained model to generate text\n",
        "new_text = generate(torch.zeros(1, 1, dtype=torch.long), seq_len=100)\n",
        "print(new_text)\n",
        "print(decode(new_text.squeeze().numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxHyDoZ8xa-t",
        "outputId": "531c462b-d5ce-4a7a-f2e4-d9c7aad2743e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0, 25, 60, 15, 19, 27, 61, 34, 35, 42, 40, 49, 49, 17, 51, 50, 12, 35,\n",
            "          7, 11,  8, 50, 49, 28, 38,  7, 23, 54, 25, 17, 59, 28, 30, 30, 49, 17,\n",
            "          4, 28, 43,  8, 63, 49,  7, 46, 15, 33, 59, 60, 49, 16, 62, 42, 24,  9,\n",
            "         62, 62, 17, 25, 59, 28, 40, 38, 28, 19, 37, 12, 46, 31,  7, 52, 15, 40,\n",
            "         39, 55, 59, 36, 46, 15,  9, 56, 16, 21,  7, 54, 64, 29, 14, 50,  5, 26,\n",
            "         62, 42, 34, 28, 40, 47, 56, 39, 55, 35, 42]])\n",
            "\n",
            "MvCGOwVWdbkkEml?W-;.lkPZ-KpMEuPRRkE&Pe.yk-hCUuvkDxdL3xxEMuPbZPGY?hS-nCbaquXhC3rDI-pzQBl'NxdVPbiraqWd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params=model_bigram.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "f8fYdRc_suHq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoches = 10000\n",
        "for epoch in range(epoches):\n",
        "  model_bigram.train()\n",
        "  xb, yb = get_sample(batch_size, block_size) # get random sample in each epoch\n",
        "  logits = model_bigram(xb)\n",
        "  B, T, C = logits.shape # change the shape to match the input shape of loss_fn\n",
        "  logits = logits.view(B*T, C)\n",
        "  targets = yb.view(B*T)\n",
        "  loss = loss_fn(logits, targets)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if(epoch == 0):\n",
        "    print(f'Logits shape: {logits.shape}')\n",
        "  if(epoch % 1000 == 0):\n",
        "    print(f'epoch: {epoch}, train loss: {loss.item():.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMxiNCX7taUk",
        "outputId": "ccc2380f-f167-41d1-991b-e91b3bc935bc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape: torch.Size([32, 65])\n",
            "epoch: 0, train loss: 4.92\n",
            "epoch: 1000, train loss: 4.04\n",
            "epoch: 2000, train loss: 4.02\n",
            "epoch: 3000, train loss: 3.09\n",
            "epoch: 4000, train loss: 2.89\n",
            "epoch: 5000, train loss: 2.84\n",
            "epoch: 6000, train loss: 2.91\n",
            "epoch: 7000, train loss: 2.61\n",
            "epoch: 8000, train loss: 2.75\n",
            "epoch: 9000, train loss: 2.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate use trained model to generate text\n",
        "print(decode(generate(torch.zeros(1, 1, dtype=torch.long), seq_len=500).squeeze().numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oavwDrSW_9xy",
        "outputId": "b6a1a594-2d99-499c-ee5e-7356cb53bea9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "u set\n",
            "\n",
            "'Tuce f y mWchhagen ath awo;mWCivede cr V:\n",
            "\n",
            "Su olavethizeageefotraycegought han'd.\n",
            "WAy?Q.\n",
            "LI.\n",
            "s d pl mure at;'d dsthintw.OY an ha?it\n",
            "Ther w hadend al frouigunchethimumad il: s s\n",
            "tP?. fof-Go\n",
            "TV&res.\n",
            "\n",
            "LAne t pe.\n",
            "BYO;Buce.\n",
            "\n",
            "SBRYOLOUKWJPS: sty hucoism e wiar plon w\n",
            "H-JGSothath, as bllved, hathrtan mP\n",
            "\n",
            "F'Tw, ouer Lot.OelaHEO w.\n",
            "INGOPHcituENEngZHou mb'lat\n",
            "\n",
            "Torc?ierit heshth hedamemo sat k?WA! taps marmon oofor amy!vyx-INws v3N mpt I fanes nvTr hon ch ttho o3qKqHERThotouptyor w, fail.\n",
            "Jghe, nchy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Single head self-attention"
      ],
      "metadata": {
        "id": "Xoh9aH7tw6Z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## lower triangle masking\n",
        "## to remove the influence from tokens to previous ones\n",
        "B, T, C = 4, 8, 32 # batch, context/sequence length, channel/embedding\n",
        "x = torch.rand(B, T, C)\n",
        "\n",
        "def apply_mask(wei):\n",
        "  tri = torch.tril(torch.ones(T, T))\n",
        "  wei = torch.masked_fill(wei, tri==0, float('-inf')) # for text generation\n",
        "  wei = F.softmax(wei, dim=-1)\n",
        "  return wei\n",
        "\n",
        "wei = torch.zeros(T, T)\n",
        "print(apply_mask(wei))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEVfNHJ0xZQv",
        "outputId": "b080ef7a-be8d-4a13-ab8f-11c97b832513"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## key, query and affinity matrix\n",
        "B, T, C = 4, 8, 32\n",
        "x = torch.rand(B, T, C)\n",
        "\n",
        "head_size = 16 # as embedding size\n",
        "# match each token from embedding to head that answer who I am\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "# match each token from embedding to head that ask what I want\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "k = key(x) # (B, T, head_size)\n",
        "q = query(x) # (B, T, head_size)\n",
        "\n",
        "# affinity matrix indicates the relationship between tokens\n",
        "affinity = q @ k.transpose(-2, -1) # (B, T, head) @ (B, head, T) -> (B, T, T)\n",
        "print(apply_mask(affinity)) # after apply mask and softmax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAWdAV4dxpXO",
        "outputId": "333b27ad-6139-4f32-c79b-3bfe09df4a95"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4937, 0.5063, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.3967, 0.3963, 0.2071, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.3245, 0.3005, 0.1547, 0.2204, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2849, 0.2798, 0.1079, 0.1990, 0.1283, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2095, 0.1815, 0.1066, 0.1718, 0.1195, 0.2110, 0.0000, 0.0000],\n",
            "         [0.1611, 0.1636, 0.1027, 0.1520, 0.1101, 0.1518, 0.1586, 0.0000],\n",
            "         [0.1405, 0.1399, 0.0696, 0.1324, 0.1055, 0.1166, 0.1403, 0.1552]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.5196, 0.4804, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.3663, 0.3821, 0.2516, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.3204, 0.3101, 0.1756, 0.1939, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2909, 0.2359, 0.1385, 0.1462, 0.1884, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2147, 0.1739, 0.1132, 0.1238, 0.1974, 0.1771, 0.0000, 0.0000],\n",
            "         [0.1713, 0.1498, 0.1039, 0.0916, 0.1209, 0.1456, 0.2168, 0.0000],\n",
            "         [0.1140, 0.1096, 0.0890, 0.0743, 0.1063, 0.1106, 0.1840, 0.2123]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.5480, 0.4520, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.3780, 0.3787, 0.2432, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2463, 0.2432, 0.2384, 0.2721, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1818, 0.1233, 0.1353, 0.2706, 0.2890, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1520, 0.1274, 0.1155, 0.2106, 0.2401, 0.1544, 0.0000, 0.0000],\n",
            "         [0.1644, 0.1348, 0.1233, 0.1560, 0.1865, 0.1160, 0.1191, 0.0000],\n",
            "         [0.1155, 0.1099, 0.0945, 0.1427, 0.1549, 0.1176, 0.1365, 0.1285]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.5075, 0.4925, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.3910, 0.3299, 0.2792, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2826, 0.2591, 0.2481, 0.2102, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2052, 0.2116, 0.1628, 0.1593, 0.2611, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1607, 0.1869, 0.1419, 0.1647, 0.2028, 0.1429, 0.0000, 0.0000],\n",
            "         [0.1413, 0.1491, 0.1267, 0.1418, 0.1677, 0.1531, 0.1202, 0.0000],\n",
            "         [0.1312, 0.1261, 0.1058, 0.1168, 0.1643, 0.1419, 0.1060, 0.1078]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right) V\n",
        "$$"
      ],
      "metadata": {
        "id": "VhhTmn3H9tHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B, T, head_size)\n",
        "k.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_94B-qCu-Y7t",
        "outputId": "4212cb5f-8819-48bf-c192-66c93488fbee"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9140)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q = torch.randn(B, T, head_size)\n",
        "q.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDU8YC8J-hS2",
        "outputId": "54d36ea0-d071-43aa-d891-f6f2df3bb843"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9795)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "affinity = q @ k.transpose(-2, -1) * head_size ** -0.5 # to keep the var\n",
        "affinity.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "no6fiL2m-7rC",
        "outputId": "309271cf-a9e9-416d-8f8a-68ac3c161889"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.8151)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# value and normalization\n",
        "B, T, C = 4, 8, 32\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "# match each token from embedding to head that related what can it provide\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "k = key(x)\n",
        "q = query(x)\n",
        "v = value(x)\n",
        "\n",
        "affinity = q @ k.transpose(-2, -1) / torch.sqrt(torch.tensor(head_size))\n",
        "affinity = apply_mask(affinity) # (B, T, T)\n",
        "out = affinity @ v # (B, T, head_size)\n",
        "out[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKbPcaJC3_Xd",
        "outputId": "50441e4e-f7e4-4620-9c7d-28305bcc99f2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0799, -0.4369,  0.1157,  0.2735,  0.0314,  0.2840,  0.0345,  0.4093,\n",
              "         -0.3660,  0.0103, -1.0115,  0.9016, -0.3275,  0.3434,  0.3896,  0.0851],\n",
              "        [ 0.2351, -0.3106,  0.1230,  0.1948, -0.0116,  0.3260,  0.1782,  0.1483,\n",
              "         -0.3691, -0.0219, -1.0119,  0.7793, -0.4661,  0.2363,  0.1934, -0.1231],\n",
              "        [ 0.0040, -0.2202, -0.0062,  0.0647, -0.2189,  0.1161,  0.0124, -0.0499,\n",
              "         -0.1839, -0.0039, -0.3031,  0.6311, -0.2707,  0.2361,  0.0855, -0.4001],\n",
              "        [-0.0667, -0.1820, -0.0615,  0.2115, -0.2336, -0.0343, -0.1382, -0.1585,\n",
              "         -0.1798,  0.1959, -0.0330,  0.2147, -0.1741,  0.0181, -0.0929, -0.0515],\n",
              "        [ 0.1482,  0.0084, -0.0211, -0.1298, -0.1261,  0.0053,  0.1351, -0.3450,\n",
              "         -0.2445,  0.0236, -0.2005,  0.2888, -0.5325, -0.0952, -0.0543, -0.3679],\n",
              "        [ 0.1782,  0.1300, -0.1134, -0.1933, -0.2255,  0.0343,  0.1572, -0.3720,\n",
              "         -0.1617,  0.0374, -0.2903,  0.1180, -0.4894, -0.1279, -0.0017, -0.2030],\n",
              "        [ 0.0497,  0.1030, -0.1675, -0.2850, -0.0604, -0.2022,  0.0075, -0.2755,\n",
              "         -0.0949,  0.0993,  0.1117, -0.0090, -0.4237, -0.2560,  0.0320, -0.1584],\n",
              "        [ 0.2754,  0.0088, -0.0844, -0.0719, -0.1304,  0.0074,  0.3355, -0.3615,\n",
              "         -0.1833, -0.0631, -0.1353,  0.0457, -0.4491, -0.1058, -0.2068, -0.3160]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32 # B\n",
        "block_size = 8 # T\n",
        "embedding_dims = 32 # C\n",
        "vocab_size = 65\n",
        "epoches = 5000\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "NdCID_pyJc4k"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  # single head self attention\n",
        "  def __init__(self, block_size, embedding_dims, head_size):\n",
        "    super().__init__()\n",
        "    self.query = nn.Linear(embedding_dims, head_size)\n",
        "    self.key = nn.Linear(embedding_dims, head_size)\n",
        "    self.value = nn.Linear(embedding_dims, head_size)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "\n",
        "    q = self.query(x)\n",
        "    k = self.key(x)\n",
        "    v = self.value(x)\n",
        "    affinity = q @ k.transpose(-2, -1) / torch.sqrt(torch.tensor(k.shape[-1]))\n",
        "\n",
        "    affinity = torch.masked_fill(affinity, self.tril[:T, :T] == 0, float('-inf')) # for text generation\n",
        "    affinity = F.softmax(affinity, dim=-1)\n",
        "\n",
        "    out = affinity @ v # (B, T, T) @ (B, T, head_size) -> (B, T, head_size)\n",
        "    return out"
      ],
      "metadata": {
        "id": "iXFcrCI6_dW8"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleHeadModel(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dims, block_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding = nn.Embedding(vocab_size, embedding_dims)\n",
        "    self.positon_embedding = nn.Embedding(block_size, embedding_dims)\n",
        "    self.self_attention_head = Head(block_size, embedding_dims, head_size=embedding_dims)\n",
        "    self.decoder = nn.Linear(embedding_dims, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T = x.shape\n",
        "    x = self.token_embedding(x) + self.positon_embedding(torch.arange(T, device=device)) # (B, T, C) + (T, C) -> (B, T, C)\n",
        "    x = self.self_attention_head(x) # (B, T, C)\n",
        "    x = self.decoder(x) # (B, T, vocab_size)\n",
        "    return x\n",
        "\n",
        "model_single_head = SingleHeadModel(vocab_size=vocab_size, embedding_dims=embedding_dims, block_size=block_size).to(device)"
      ],
      "metadata": {
        "id": "1EXhG592L62_"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use x(shape(B, T)) to predict next token,\n",
        "# after go through the model still use last token to predict the next token,\n",
        "# and loop to generate text at new_seq_len length\n",
        "def generate(x, new_seq_len): # x-> shape(B, T)\n",
        "  model_single_head.eval()\n",
        "  x = x.to(device)\n",
        "  for _ in range(new_seq_len):\n",
        "    with torch.inference_mode():\n",
        "      context = x[:, -block_size:] # if x exceeds block_size, only feed the last block_size tokens to the model\n",
        "      logits = model_single_head(context) # (B, T, vocab_size)\n",
        "      logits_last = logits[:,-1,:] # (B, vocab_size)\n",
        "      logits_last_prob = F.softmax(logits_last, dim=-1) # (B, vocab_size)\n",
        "      token_next = torch.multinomial(logits_last_prob, num_samples=1) # (B, 1)\n",
        "      x = torch.cat((x, token_next), dim=1) # (B, T+1)\n",
        "  return x"
      ],
      "metadata": {
        "id": "OeDUEG-gRfJz"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params=model_single_head.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "VawWeMWzH46s"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epoches):\n",
        "  model_single_head.train()\n",
        "  xb, yb = get_sample(batch_size, block_size) # get random sample in each epoch\n",
        "  xb, yb = xb.to(device), yb.to(device)\n",
        "  logits = model_single_head(xb)\n",
        "  B, T, C = logits.shape # change the shape to match the input shape of loss_fn\n",
        "  logits = logits.view(B*T, C)\n",
        "  targets = yb.view(B*T)\n",
        "  loss = loss_fn(logits, targets)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if(epoch == 0):\n",
        "    print(f'Logits shape: {logits.shape}')\n",
        "  if(epoch % 1000 == 0):\n",
        "    print(f'epoch: {epoch}, train loss: {loss.item():.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gke25i7jH_hs",
        "outputId": "7b9581c1-0f0d-4fd5-d8c3-6b6cbd4a4c91"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape: torch.Size([256, 65])\n",
            "epoch: 0, train loss: 4.13\n",
            "epoch: 1000, train loss: 2.39\n",
            "epoch: 2000, train loss: 2.54\n",
            "epoch: 3000, train loss: 2.27\n",
            "epoch: 4000, train loss: 2.41\n",
            "epoch: 5000, train loss: 2.37\n",
            "epoch: 6000, train loss: 2.39\n",
            "epoch: 7000, train loss: 2.38\n",
            "epoch: 8000, train loss: 2.28\n",
            "epoch: 9000, train loss: 2.39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nd4QqMHpQI2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate use trained model to generate text\n",
        "print(decode(generate(torch.zeros(1, 1, dtype=torch.long), new_seq_len=500).squeeze().cpu().numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVdhwazBLMWQ",
        "outputId": "05245259-56af-4f5f-f49d-20dfc294876e"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "PERTANERI KIUSins ms innen,\n",
            "Sis, ise ito mee the omaber nes doucrothu gem eperelis our'!\n",
            "\n",
            "Wopere to thef ter fo heamal chag ds: nde 't thago!\n",
            "Whichould by eave wis horu,\n",
            "QUUKIUS:\n",
            "O-tillepo mitothyu?\n",
            "\n",
            "PELORLUKE IZARENRCHENTESS:\n",
            "Whath mathy, dse an end al mane 'se;\n",
            "USl:\n",
            "An me;\n",
            "G RCIFIO:\n",
            "Oy, yofa hot haer spans ayt dom?\n",
            "\n",
            "Fes I thereswanthe?-\n",
            "Heckncere pu'd ar.\n",
            "\n",
            "Hed foith mnpr tay aknifet\n",
            "Bent pe:\n",
            "He, RD IURESl: hath dthivowupnsem o's. ard prsithe lifucay hourig men.\n",
            "\n",
            "\n",
            "foud thouth alwh ay wopron, th\n"
          ]
        }
      ]
    }
  ]
}