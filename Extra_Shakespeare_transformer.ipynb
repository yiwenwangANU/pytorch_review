{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMTeiKOidSzJFC4DF9Cra8M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yiwenwangANU/pytorch_review/blob/main/Extra_Shakespeare_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import os\n",
        "from pathlib import Path\n",
        "import requests"
      ],
      "metadata": {
        "id": "I4gOLJiLRYzy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(\"GPU ready!\")\n",
        "else:\n",
        "  device = \"cpu\"\n",
        "  print(\"GPU not available!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGyCf3JSTMTf",
        "outputId": "8180cc5c-156b-4c72-dbda-5922b72fc3a1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data download and explore"
      ],
      "metadata": {
        "id": "7xIk_As9rnN7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RP-67rJQ2Ia",
        "outputId": "aac84193-bdeb-4771-ec57-f53b9c9a1188"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading training data to /root/data/input.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.06M/1.06M [00:00<00:00, 132MB/s]\n"
          ]
        }
      ],
      "source": [
        "data_dir = Path.home() / 'data'\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "data_dir = os.path.join(data_dir, \"input.txt\")\n",
        "if os.path.exists(data_dir):\n",
        "  print(\"Data ready.\")\n",
        "else:\n",
        "  print(f\"Downloading training data to {data_dir}\")\n",
        "  url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "  torch.hub.download_url_to_file(url, data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(data_dir, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "    print(f\"length of dataset in characters: {len(text)}\")\n",
        "    print('First 300 chars:')\n",
        "    print(text[:300])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvqsUA7vSodf",
        "outputId": "58edde80-57bc-4da1-9966-4b7450694e6d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1115394\n",
            "First 300 chars:\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenization"
      ],
      "metadata": {
        "id": "3mfoK1i8r11M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(list(set(text)))\n",
        "vocab_size = len(vocab)\n",
        "print(f'Number of unique chars: {vocab_size}')\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuK1Lw5kUavB",
        "outputId": "8b75db45-763f-4b59-9457-78b6e6c6d072"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique chars: 65\n",
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "charTotoken = {char:i for i,char in enumerate(vocab)}\n",
        "tokenTochar = {i:char for i,char in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "_dJSU6ztYafO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(input):\n",
        "  return [charTotoken[char] for char in input]\n",
        "\n",
        "def decode(input):\n",
        "  return ''.join([tokenTochar[token] for token in input])"
      ],
      "metadata": {
        "id": "nw8M80vrUU3n"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encode('First Citizen:'))\n",
        "print(decode(encode('First Citizen:')))"
      ],
      "metadata": {
        "id": "YEuMJXdrbDSA",
        "outputId": "cc6be105-458f-4fb6-c096-d0d8c6b1cc25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10]\n",
            "First Citizen:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check other tokenizer like gpt-tokenizer"
      ],
      "metadata": {
        "id": "MrmzVgsabEL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape)\n",
        "print(data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcqcRGKOptr0",
        "outputId": "bdb348ef-78ae-4b3f-8400-f79164974466"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394])\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train test split"
      ],
      "metadata": {
        "id": "ABWZPmCwqOm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "Rr3679IUsCnS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "\n",
        "def get_sample(batch_size, block_size):\n",
        "  idxes = torch.randint(len(train_data)-block_size, (batch_size,))\n",
        "  xb = torch.stack([train_data[idx:idx+block_size] for idx in idxes])\n",
        "  yb = torch.stack([train_data[idx+1: idx+block_size+1] for idx in idxes])\n",
        "  return xb, yb\n",
        "\n",
        "xb, yb = get_sample(batch_size, block_size)\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "id": "zj71XSdR3Il7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87ccc55f-f99b-43b5-f071-42fa09001045"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[57,  1, 46, 47, 57,  1, 50, 53],\n",
            "        [ 1, 58, 46, 43, 56, 43,  1, 41],\n",
            "        [17, 26, 15, 17, 10,  0, 32, 53],\n",
            "        [57, 58,  6,  1, 61, 47, 58, 46]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[ 1, 46, 47, 57,  1, 50, 53, 60],\n",
            "        [58, 46, 43, 56, 43,  1, 41, 39],\n",
            "        [26, 15, 17, 10,  0, 32, 53,  1],\n",
            "        [58,  6,  1, 61, 47, 58, 46,  0]])\n",
            "----\n",
            "when input is [57] the target: 1\n",
            "when input is [57, 1] the target: 46\n",
            "when input is [57, 1, 46] the target: 47\n",
            "when input is [57, 1, 46, 47] the target: 57\n",
            "when input is [57, 1, 46, 47, 57] the target: 1\n",
            "when input is [57, 1, 46, 47, 57, 1] the target: 50\n",
            "when input is [57, 1, 46, 47, 57, 1, 50] the target: 53\n",
            "when input is [57, 1, 46, 47, 57, 1, 50, 53] the target: 60\n",
            "when input is [1] the target: 58\n",
            "when input is [1, 58] the target: 46\n",
            "when input is [1, 58, 46] the target: 43\n",
            "when input is [1, 58, 46, 43] the target: 56\n",
            "when input is [1, 58, 46, 43, 56] the target: 43\n",
            "when input is [1, 58, 46, 43, 56, 43] the target: 1\n",
            "when input is [1, 58, 46, 43, 56, 43, 1] the target: 41\n",
            "when input is [1, 58, 46, 43, 56, 43, 1, 41] the target: 39\n",
            "when input is [17] the target: 26\n",
            "when input is [17, 26] the target: 15\n",
            "when input is [17, 26, 15] the target: 17\n",
            "when input is [17, 26, 15, 17] the target: 10\n",
            "when input is [17, 26, 15, 17, 10] the target: 0\n",
            "when input is [17, 26, 15, 17, 10, 0] the target: 32\n",
            "when input is [17, 26, 15, 17, 10, 0, 32] the target: 53\n",
            "when input is [17, 26, 15, 17, 10, 0, 32, 53] the target: 1\n",
            "when input is [57] the target: 58\n",
            "when input is [57, 58] the target: 6\n",
            "when input is [57, 58, 6] the target: 1\n",
            "when input is [57, 58, 6, 1] the target: 61\n",
            "when input is [57, 58, 6, 1, 61] the target: 47\n",
            "when input is [57, 58, 6, 1, 61, 47] the target: 58\n",
            "when input is [57, 58, 6, 1, 61, 47, 58] the target: 46\n",
            "when input is [57, 58, 6, 1, 61, 47, 58, 46] the target: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = nn.Embedding(num_embeddings=10, embedding_dim=3)\n",
        "\n",
        "# Input tensor (indices of words or items)\n",
        "input_indices = torch.tensor([1, 5, 7, 7])\n",
        "\n",
        "# Get embeddings\n",
        "output = embedding(input_indices)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "ZdbKBaK91rL1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9ad65bf-e680-4965-e353-af6806fc3d9e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.6047,  1.7878, -0.4780],\n",
            "        [ 1.7174,  0.3189, -0.4245],\n",
            "        [ 0.3211, -0.8798, -0.6011],\n",
            "        [ 0.3211, -0.8798, -0.6011]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bigram Model"
      ],
      "metadata": {
        "id": "FClvLSICxN39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# A Bigram Model predicts the next token based only on the previous token\n",
        "# (i.e., it learns a probability distribution of token transitions).\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    # each token directly reads off the logits for the next token from a lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "  def forward(self, x):\n",
        "    return self.token_embedding_table(x)\n",
        "\n",
        "model_bigram = BigramLanguageModel(vocab_size)"
      ],
      "metadata": {
        "id": "6J0VseFUZ0d8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the last token in x to predict next token\n",
        "# and loop to generate text at seq_len length\n",
        "torch.manual_seed(42)\n",
        "def generate(x, seq_len): # x-> shape(Batch size, 1)\n",
        "  for _ in range(seq_len):\n",
        "    logits = model_bigram(x) # shape(Batch size, Sequence length, Vocabulary size)\n",
        "    logits_last = logits[:,-1,:] # shape(Batch size, Vocabulary size)\n",
        "    logits_last_prob = F.softmax(logits_last, dim=-1) # shape(Batch size, Vocabulary size)\n",
        "    token_next = torch.multinomial(logits_last_prob, num_samples=1) # shape(B, 1)\n",
        "    x = torch.cat((x, token_next), dim=1) # (B, T+1)\n",
        "  return x"
      ],
      "metadata": {
        "id": "VBMGVktAtpUu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate use untrained model to generate text\n",
        "new_text = generate(torch.zeros(1, 1, dtype=torch.long), seq_len=100)\n",
        "print(new_text)\n",
        "print(decode(new_text.squeeze().numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxHyDoZ8xa-t",
        "outputId": "7fdd624a-6b7f-424c-9a13-5cf270e9c32f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0, 25, 60, 15, 19, 27, 61, 34, 35, 42, 40, 49, 49, 17, 51, 50, 12, 35,\n",
            "          7, 11,  8, 50, 49, 28, 38,  7, 23, 54, 25, 17, 59, 28, 30, 30, 49, 17,\n",
            "          4, 28, 43,  8, 63, 49,  7, 46, 15, 33, 59, 60, 49, 16, 62, 42, 24,  9,\n",
            "         62, 62, 17, 25, 59, 28, 40, 38, 28, 19, 37, 12, 46, 31,  7, 52, 15, 40,\n",
            "         39, 55, 59, 36, 46, 15,  9, 56, 16, 21,  7, 54, 64, 29, 14, 50,  5, 26,\n",
            "         62, 42, 34, 28, 40, 47, 56, 39, 55, 35, 42]])\n",
            "\n",
            "MvCGOwVWdbkkEml?W-;.lkPZ-KpMEuPRRkE&Pe.yk-hCUuvkDxdL3xxEMuPbZPGY?hS-nCbaquXhC3rDI-pzQBl'NxdVPbiraqWd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params=model_bigram.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "f8fYdRc_suHq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoches = 10000\n",
        "for epoch in range(epoches):\n",
        "  model_bigram.train()\n",
        "  xb, yb = get_sample(batch_size, block_size) # get random sample in each epoch\n",
        "  logits = model_bigram(xb)\n",
        "  B, T, C = logits.shape # change the shape to match the input shape of loss_fn\n",
        "  logits = logits.view(B*T, C)\n",
        "  targets = yb.view(B*T)\n",
        "  loss = loss_fn(logits, targets)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if(epoch == 0):\n",
        "    print(f'Logits shape: {logits.shape}')\n",
        "  if(epoch % 1000 == 0):\n",
        "    print(f'epoch: {epoch}, train loss: {loss.item():.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMxiNCX7taUk",
        "outputId": "fb5ddf99-f359-4e2d-fe53-3d5c00ac10dc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape: torch.Size([32, 65])\n",
            "epoch: 0, train loss: 4.92\n",
            "epoch: 1000, train loss: 4.04\n",
            "epoch: 2000, train loss: 4.02\n",
            "epoch: 3000, train loss: 3.09\n",
            "epoch: 4000, train loss: 2.89\n",
            "epoch: 5000, train loss: 2.84\n",
            "epoch: 6000, train loss: 2.91\n",
            "epoch: 7000, train loss: 2.61\n",
            "epoch: 8000, train loss: 2.75\n",
            "epoch: 9000, train loss: 2.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate use trained model to generate text\n",
        "print(decode(generate(torch.zeros(1, 1, dtype=torch.long), seq_len=500).squeeze().numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oavwDrSW_9xy",
        "outputId": "2a55d341-9cdd-4427-808d-d0761072d712"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "u set\n",
            "\n",
            "'Tuce f y mWchhagen ath awo;mWCivede cr V:\n",
            "\n",
            "Su olavethizeageefotraycegought han'd.\n",
            "WAy?Q.\n",
            "LI.\n",
            "s d pl mure at;'d dsthintw.OY an ha?it\n",
            "Ther w hadend al frouigunchethimumad il: s s\n",
            "tP?. fof-Go\n",
            "TV&res.\n",
            "\n",
            "LAne t pe.\n",
            "BYO;Buce.\n",
            "\n",
            "SBRYOLOUKWJPS: sty hucoism e wiar plon w\n",
            "H-JGSothath, as bllved, hathrtan mP\n",
            "\n",
            "F'Tw, ouer Lot.OelaHEO w.\n",
            "INGOPHcituENEngZHou mb'lat\n",
            "\n",
            "Torc?ierit heshth hedamemo sat k?WA! taps marmon oofor amy!vyx-INws v3N mpt I fanes nvTr hon ch ttho o3qKqHERThotouptyor w, fail.\n",
            "Jghe, nchy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Single head self-attention"
      ],
      "metadata": {
        "id": "Xoh9aH7tw6Z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## lower triangle masking\n",
        "## to remove the influence from tokens to previous ones\n",
        "B, T, C = 4, 8, 32 # batch, context/sequence length, channel/embedding\n",
        "x = torch.rand(B, T, C)\n",
        "\n",
        "def apply_mask(wei):\n",
        "  tri = torch.tril(torch.ones(T, T))\n",
        "  wei = torch.masked_fill(wei, tri==0, float('-inf')) # for text generation\n",
        "  wei = F.softmax(wei, dim=-1)\n",
        "  return wei\n",
        "\n",
        "wei = torch.zeros(T, T)\n",
        "print(apply_mask(wei))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEVfNHJ0xZQv",
        "outputId": "4130b10b-d3e6-4af1-8161-7f66e03d62c2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## key, query and affinity matrix\n",
        "B, T, C = 4, 8, 32\n",
        "x = torch.rand(B, T, C)\n",
        "\n",
        "head_size = 16 # as embedding size\n",
        "# match each token from embedding to head that answer who I am\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "# match each token from embedding to head that ask what I want\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "k = key(x) # (B, T, head_size)\n",
        "q = query(x) # (B, T, head_size)\n",
        "\n",
        "# affinity matrix indicates the relationship between tokens\n",
        "affinity = q @ k.transpose(-2, -1) # (B, T, head) @ (B, head, T) -> (B, T, T)\n",
        "print(apply_mask(affinity)) # after apply mask and softmax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAWdAV4dxpXO",
        "outputId": "a1b9dfab-91a1-48fc-e4ed-a5f9e8f3ecc7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.5442, 0.4558, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.3460, 0.3800, 0.2740, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2451, 0.2649, 0.2603, 0.2296, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2288, 0.2166, 0.1943, 0.1776, 0.1826, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1842, 0.1937, 0.1519, 0.1461, 0.1362, 0.1879, 0.0000, 0.0000],\n",
            "         [0.1726, 0.1676, 0.1344, 0.1121, 0.1356, 0.1398, 0.1378, 0.0000],\n",
            "         [0.1468, 0.1503, 0.1356, 0.1077, 0.0956, 0.1248, 0.1022, 0.1369]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.5692, 0.4308, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.3533, 0.3533, 0.2934, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2417, 0.2473, 0.2594, 0.2516, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.3048, 0.1623, 0.1902, 0.1687, 0.1739, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2225, 0.1218, 0.1632, 0.1382, 0.1574, 0.1970, 0.0000, 0.0000],\n",
            "         [0.1556, 0.1446, 0.1434, 0.1393, 0.1256, 0.1642, 0.1273, 0.0000],\n",
            "         [0.1738, 0.1307, 0.1195, 0.1114, 0.1096, 0.1392, 0.0938, 0.1221]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4387, 0.5613, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.3319, 0.3909, 0.2772, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1844, 0.3162, 0.2085, 0.2909, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1663, 0.2399, 0.1781, 0.2105, 0.2053, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1737, 0.1701, 0.1498, 0.1443, 0.1718, 0.1902, 0.0000, 0.0000],\n",
            "         [0.1268, 0.1374, 0.1284, 0.1748, 0.1501, 0.1712, 0.1114, 0.0000],\n",
            "         [0.0971, 0.1455, 0.1105, 0.1232, 0.1176, 0.1650, 0.1040, 0.1371]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4858, 0.5142, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.3512, 0.3117, 0.3371, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2226, 0.2838, 0.2469, 0.2467, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1596, 0.2128, 0.1946, 0.1820, 0.2510, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1577, 0.1705, 0.1619, 0.1799, 0.1837, 0.1463, 0.0000, 0.0000],\n",
            "         [0.1215, 0.1302, 0.1082, 0.1289, 0.1841, 0.1100, 0.2171, 0.0000],\n",
            "         [0.0905, 0.1134, 0.1191, 0.1075, 0.1393, 0.1186, 0.2204, 0.0912]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right) V\n",
        "$$"
      ],
      "metadata": {
        "id": "VhhTmn3H9tHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B, T, head_size)\n",
        "k.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_94B-qCu-Y7t",
        "outputId": "d6fa01b3-fb08-41de-b5f9-febb4ececa0f"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.8981)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q = torch.randn(B, T, head_size)\n",
        "q.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDU8YC8J-hS2",
        "outputId": "8b08de21-bf72-40ee-83d3-b71ae220d715"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9625)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "affinity = q @ k.transpose(-2, -1) * head_size ** -0.5 # to keep the var\n",
        "affinity.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "no6fiL2m-7rC",
        "outputId": "aa5c3a2a-e8dd-40b5-e305-35818cc7c638"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9931)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# value and normalization\n",
        "B, T, C = 4, 8, 32\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "# match each token from embedding to head that related what can it provide\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "k = key(x)\n",
        "q = query(x)\n",
        "v = value(x)\n",
        "\n",
        "affinity = q @ k.transpose(-2, -1) / torch.sqrt(torch.tensor(head_size))\n",
        "affinity = apply_mask(affinity) # (B, T, T)\n",
        "out = affinity @ v # (B, T, head_size)\n",
        "out[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKbPcaJC3_Xd",
        "outputId": "cb7af136-ad62-4f28-952a-c3dcbf4b81dd"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1695, -0.8718, -0.6291, -0.5285, -1.7377,  0.1358,  0.9197,  0.0402,\n",
              "         -0.3905, -0.4409,  0.5177, -0.4440, -1.5632,  0.8519, -0.1724, -0.5118],\n",
              "        [-0.0971, -1.0301, -0.4062, -0.3271, -0.3663,  0.1677,  0.5421,  0.2114,\n",
              "         -0.3842, -0.2818, -0.2434, -0.5673, -1.3612,  0.4215, -0.1127, -0.1624],\n",
              "        [-0.0748, -0.8155, -0.0661, -0.1874, -0.2096,  0.4434,  0.5427,  0.4432,\n",
              "         -0.6145, -0.4691, -0.0510, -0.5048, -1.1601,  0.3147, -0.2537, -0.0507],\n",
              "        [-0.0241, -0.0613,  0.2207, -0.2147, -0.3649,  0.2226,  0.6861,  0.5079,\n",
              "         -0.3419, -0.3785, -0.0740, -0.1482, -0.8606,  0.1701, -0.3805,  0.1540],\n",
              "        [-0.0202, -0.0729,  0.1905, -0.3804, -0.1995,  0.0709,  0.5346,  0.6225,\n",
              "         -0.2192, -0.2154,  0.1756, -0.0710, -0.6566,  0.1781, -0.0822,  0.1677],\n",
              "        [-0.1557, -0.3369,  0.0856, -0.4102, -0.0350,  0.0331,  0.4870,  0.4469,\n",
              "          0.0250, -0.3400,  0.0077, -0.1549, -0.6103,  0.1962, -0.0810, -0.0530],\n",
              "        [-0.2464, -0.4073,  0.1663, -0.5290,  0.2550, -0.1015,  0.3597,  0.5213,\n",
              "          0.1235, -0.1334,  0.2807, -0.2670, -0.4325,  0.1649,  0.1656,  0.0515],\n",
              "        [-0.1716, -0.2921,  0.0921, -0.4531,  0.0264, -0.2186,  0.4040,  0.4253,\n",
              "          0.1314, -0.1359,  0.1140, -0.2111, -0.4466,  0.1476,  0.1121,  0.1055]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  # single head self attention\n",
        "  def __init__(self, block_size, embedding_dims, head_size):\n",
        "    super().__init__()\n",
        "    self.query = nn.Linear(embedding_dims, head_size)\n",
        "    self.key = nn.Linear(embedding_dims, head_size)\n",
        "    self.value = nn.Linear(embedding_dims, head_size)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "\n",
        "    q = self.query(x)\n",
        "    k = self.key(x)\n",
        "    v = self.value(x)\n",
        "    affinity = q @ k.transpose(-2, -1) / torch.sqrt(torch.tensor(k.shape[-1]))\n",
        "\n",
        "    affinity = torch.masked_fill(affinity, self.tril[:T, :T] == 0, float('-inf')) # for text generation\n",
        "    affinity = F.softmax(affinity, dim=-1)\n",
        "\n",
        "    out = affinity @ v # (B, T, T) @ (B, T, head_size) -> (B, T, head_size)\n",
        "    return out"
      ],
      "metadata": {
        "id": "iXFcrCI6_dW8"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleHeadModel(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dims, block_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding = nn.Embedding(vocab_size, embedding_dims)\n",
        "    self.positon_embedding = nn.Embedding(block_size, embedding_dims)\n",
        "    self.self_attention_head = Head(block_size, embedding_dims, head_size=embedding_dims)\n",
        "    self.docoder = nn.Linear(embedding_dims, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T = x.shape\n",
        "    x = self.token_embedding(x) + self.positon_embedding(torch.arange(T)) # (B, T, C) + (T, C) -> (B, T, C)\n",
        "    x = self.self_attention_head(x) # (B, T, C)\n",
        "    x = self.decoder(x) # (B, T, vocab_size)\n",
        "    return x\n",
        "\n",
        "model_single_head = SingleHeadModel(vocab_size=vocab_size, embedding_dims=32, block_size=8).to(device)"
      ],
      "metadata": {
        "id": "1EXhG592L62_"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use x(shape(B, T)) to predict next token\n",
        "# and loop to generate text at new_seq_len length\n",
        "torch.manual_seed(42)\n",
        "def generate(x, new_seq_len): # x-> shape(B, T)\n",
        "  for _ in range(new_seq_len):\n",
        "    logits = model_single_head(x) # (B, T, vocab_size)\n",
        "    logits_last = logits[:,-1,:] # (B, vocab_size)\n",
        "    logits_last_prob = F.softmax(logits_last, dim=-1) # (B, vocab_size)\n",
        "    token_next = torch.multinomial(logits_last_prob, num_samples=1) # (B, 1)\n",
        "    x = torch.cat((x, token_next), dim=1) # (B, T+1)\n",
        "  return x"
      ],
      "metadata": {
        "id": "OeDUEG-gRfJz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}